{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6db2ffc",
   "metadata": {},
   "source": [
    "This notebook evaluates different LLM-as-a-judge strategies for LM Compass.\n",
    "\n",
    "Reference material: https://docs.google.com/document/d/1vKkgJj6Tj-gSZ-1LUBvNQQ34gatz0RaRCWAMDegFozU/edit?tab=t.0#heading=h.jo95wu3e9n0z (Prompt­-based Rubric Scoring, Multi-Agent Self Reflection, Rationale‑Based Self‑Critique Loops)\n",
    "\n",
    "Also see our proposed algorithm for judging: https://docs.google.com/document/d/1oDZiobHY0ze7zyKv1oRim8qLS9VL1oiLWeWElbhV6RI/edit?usp=sharing\n",
    "\n",
    "The goal is to compare various methods against each other and against simply using a single model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da32d6",
   "metadata": {},
   "source": [
    "General prompt -> evaluation flow:\n",
    "0. Select n model candidates M (n = 2 to 4)\n",
    "1. Call OpenRouter API on initial input query Q to M candidates (in parallel, async function required probably)\n",
    "2. Store all responses R_0..R_n\n",
    "3. Pick an evaluation method\n",
    "4. Initialize judge(s) based on evaluation method\n",
    "5. Compare the judges evaluation to a baseline LLM (e.g. Base GPT-4o vs. GPT-4o Judge)\n",
    "\n",
    "Example of evaluation comparison\n",
    "1. User submits query\n",
    "2. Query gets passed to GPT-4o and Deepseek (A & B)\n",
    "3. We pick our proposed algorithm for evaluation (see above)\n",
    "3.1 Response A gets sent to Judge B. Response B gets sent to Judge A.\n",
    "3.2 Given a generic judging prompt, they determine a score\n",
    "3.3 The returned response is the response with the higher score (as long as it passes threshold, see above linked document)\n",
    "4. Return the 'winning' response\n",
    "5. Find metrics or reasons for effectiveness of this approach\n",
    "6. Repeat for other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23bfbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %conda install -c conda-forge pandas notebook python-dotenv -y\n",
    "# %pip install openai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74acda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_TITLE = \"LM Compass\"\n",
    "SITE_URL = \"https://localhost:3000\"\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in .env file or environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9189a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=OPENROUTER_API_KEY,\n",
    "  default_headers={\n",
    "    \"HTTP-Referer\": SITE_URL,\n",
    "    \"X-Title\": PROJECT_TITLE,\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b00259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 2-4 models for initial response generation (See openrouter.ai for models)\n",
    "candidate_models = [\n",
    "    'openai/gpt-4o-mini',\n",
    "    'anthropic/claude-3.5-sonnet',\n",
    "    'tngtech/deepseek-r1t2-chimera:free'\n",
    "    # Add or remove models as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eeda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "user_query = \"Explain the concept of quantum entanglement in simple terms for a high school student.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone-evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
