{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6db2ffc",
   "metadata": {},
   "source": [
    "This notebook evaluates different LLM-as-a-judge strategies for LM Compass.\n",
    "\n",
    "Reference material: https://docs.google.com/document/d/1vKkgJj6Tj-gSZ-1LUBvNQQ34gatz0RaRCWAMDegFozU/edit?tab=t.0#heading=h.jo95wu3e9n0z (Prompt­-based Rubric Scoring, Multi-Agent Self Reflection, Rationale‑Based Self‑Critique Loops)\n",
    "\n",
    "Also see our proposed algorithm for judging: https://docs.google.com/document/d/1oDZiobHY0ze7zyKv1oRim8qLS9VL1oiLWeWElbhV6RI/edit?usp=sharing\n",
    "\n",
    "The goal is to compare various methods against each other and against simply using a single model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da32d6",
   "metadata": {},
   "source": [
    "General prompt -> evaluation flow:\n",
    "0. Select n model candidates M (n = 2 to 4)\n",
    "1. Call OpenRouter API on initial input query Q to M candidates (in parallel, async function required probably)\n",
    "2. Store all responses R_0..R_n\n",
    "3. Pick an evaluation method\n",
    "4. Initialize judge(s) based on evaluation method\n",
    "5. Compare the judges evaluation to a baseline LLM (e.g. Base GPT-4o vs. GPT-4o Judge)\n",
    "\n",
    "Example of evaluation comparison\n",
    "1. User submits query\n",
    "2. Query gets passed to GPT-4o and Deepseek (A & B)\n",
    "3. We pick our proposed algorithm for evaluation (see above)\n",
    "3.1 Response A gets sent to Judge B. Response B gets sent to Judge A.\n",
    "3.2 Given a generic judging prompt, they determine a score\n",
    "3.3 The returned response is the response with the higher score (as long as it passes threshold, see above linked document)\n",
    "4. Return the 'winning' response\n",
    "5. Find metrics or reasons for effectiveness of this approach\n",
    "6. Repeat for other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23bfbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74acda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in .env file or environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9189a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=OPENROUTER_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b00259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL AVALIABLE MODELS FOR TESTING\n",
    "# *to add models, use the screen name for the model from OpenRouter as the key and\n",
    "#  the actual name used in the api as the value\n",
    "\n",
    "candidate_models = {\n",
    "    # Free Models\n",
    "    \"MiniMax: MiniMax M2 (free)\"          : \"minimax/minimax-m2:free\",\n",
    "    \"TNG: DeepSeek R1T2 Chimera (free)\"   : \"tngtech/deepseek-r1t2-chimera:free\",\n",
    "    \"Meta: Llama 3.3 70B Instruct (free)\" : \"meta-llama/llama-3.3-70b-instruct:free\",\n",
    "    \"OpenAI: gpt-oss-20b (free)\"          : \"openai/gpt-oss-20b:free\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eeda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY FUNCTIONS\n",
    "\n",
    "async def query_model(model: str, query: str, role=\"user\"):\n",
    "    \"\"\"\n",
    "    Queries a single model using the models in 'candidate_models'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=candidate_models[model],\n",
    "            messages=[{\"role\" : role, \"content\" : query}],\n",
    "            temperature=1\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        return model, content\n",
    "    except Exception as e:\n",
    "        return model, str(e)\n",
    "\n",
    "async def query_models(models: list[str], queries: list[str], role=\"user\"):\n",
    "    \"\"\"\n",
    "    Queries multiple models asychronously\n",
    "    \"\"\"\n",
    "    coroutines = [query_model(models[i], queries[i], role=role) for i in range(len(models))]\n",
    "    results = await asyncio.gather(*coroutines)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EACH MODEL FOR THEIR ANSWER TO THE USER PROMPT\n",
    "\n",
    "user_query = \"What color is grass?\"\n",
    "models_to_use = [\"MiniMax: MiniMax M2 (free)\", \"Meta: Llama 3.3 70B Instruct (free)\", \"TNG: DeepSeek R1T2 Chimera (free)\"]\n",
    "result = await query_models(models_to_use, [user_query]*len(models_to_use))\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING THE QUERIES FOR EACH MODEL TO EVALUATE EACH OTHER\n",
    "\n",
    "scoring_query = lambda answer : f\"\"\"\\\n",
    "Question: {user_query}\n",
    "Answer: {answer}\n",
    "\n",
    "Please rate the answer to the question and give it a score between 0 and 100. End your output with just the score on a new line.\\\n",
    "\"\"\"\n",
    "\n",
    "new_models_to_use = []\n",
    "new_queries_to_use = []\n",
    "for model1 in models_to_use:\n",
    "    for model2, answer in result:\n",
    "        if model1 != model2:\n",
    "            new_models_to_use.append(model1)\n",
    "            new_queries_to_use.append(scoring_query(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d740aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EACH MODEL TO EVALUATE EACH OTHER MODEL'S ANSWER\n",
    "\n",
    "scoring_results = await query_models(new_models_to_use, new_queries_to_use)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
