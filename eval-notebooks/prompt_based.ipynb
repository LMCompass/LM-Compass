{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23bfbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import asyncio\n",
    "import re\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74acda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in .env file or environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9189a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=OPENROUTER_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b00259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL AVAILABLE MODELS FOR TESTING\n",
    "# *to add models, use the screen name for the model from OpenRouter as the key and\n",
    "# the actual name used in the api as the value\n",
    "# *comment out models that you don't want to be used\n",
    "\n",
    "candidate_models = [\n",
    "    # PAID\n",
    "    # {\"name\": \"Anthropic: Claude Sonnet 4.5\", \"openrouter\": \"anthropic/claude-sonnet-4.5\"},\n",
    "    # {\"name\": \"OpenAI: GPT-4o\", \"openrouter\": \"openai/gpt-4o\"},\n",
    "    # {\"name\": \"OpenAI: GPT-4o Mini\", \"openrouter\": \"openai/gpt-4o-mini\"}\n",
    "    # FREE\n",
    "    # {\"name\": \"MiniMax: MiniMax M2 (free)\", \"openrouter\": \"minimax/minimax-m2:free\"}, # Not working for me (OWEN)\n",
    "    {\"name\": \"TNG: DeepSeek R1T2 Chimera (free)\", \"openrouter\": \"tngtech/deepseek-r1t2-chimera:free\"},\n",
    "    {\"name\": \"Meta: Llama 3.3 70B Instruct (free)\", \"openrouter\": \"meta-llama/llama-3.3-70b-instruct:free\"},\n",
    "    {\"name\": \"OpenAI: gpt-oss-20b (free)\", \"openrouter\": \"openai/gpt-oss-20b:free\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72eeda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY FUNCTIONS\n",
    "\n",
    "async def query_model(model_name: str, query: str, role=\"user\"):\n",
    "    \"\"\"\n",
    "    Queries a single model using the models in 'candidate_models'\n",
    "    Args:\n",
    "        model_name: The name of the model (from candidate_models \"name\" field)\n",
    "    Returns: dict with keys 'model' and 'response'\n",
    "    \"\"\"\n",
    "    model_dict = next((m for m in candidate_models if m[\"name\"] == model_name), None)\n",
    "    if model_dict is None:\n",
    "        return {\"model\": model_name, \"response\": f\"Error: Model '{model_name}' not found in candidate_models\"}\n",
    "    \n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=model_dict[\"openrouter\"],\n",
    "            messages=[{\"role\" : role, \"content\" : query}],\n",
    "            temperature=1\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        return {\"model\": model_name, \"response\": content}\n",
    "    except Exception as e:\n",
    "        return {\"model\": model_name, \"response\": str(e)}\n",
    "\n",
    "async def query_models(model_names: list[str], queries: list[str], role=\"user\"):\n",
    "    \"\"\"\n",
    "    Queries multiple models asynchronously\n",
    "    Args:\n",
    "        model_names: List of model names (from candidate_models \"name\" field)\n",
    "    \"\"\"\n",
    "    coroutines = [query_model(model_names[i], queries[i], role=role) for i in range(len(model_names))]\n",
    "    results = await asyncio.gather(*coroutines)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ae6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON PARSER\n",
    "\n",
    "def extract_outermost_json(text):\n",
    "    \"\"\"\n",
    "    Extracts the outermost JSON object from arbitrary text.\n",
    "    Returns the parsed JSON (dict/list) or raises ValueError if no valid JSON found.\n",
    "    \"\"\"\n",
    "\n",
    "    start = None\n",
    "    depth = 0\n",
    "    in_string = False\n",
    "    escape = False\n",
    "\n",
    "    for i, ch in enumerate(text):\n",
    "        if escape:\n",
    "            escape = False\n",
    "            continue\n",
    "\n",
    "        if ch == \"\\\\\":\n",
    "            escape = True\n",
    "            continue\n",
    "\n",
    "        if ch == '\"' and not escape:\n",
    "            in_string = not in_string\n",
    "            continue\n",
    "\n",
    "        if not in_string:\n",
    "            if ch == \"{\":\n",
    "                if depth == 0:\n",
    "                    start = i\n",
    "                depth += 1\n",
    "            elif ch == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0 and start is not None:\n",
    "                    candidate = text[start:i+1]\n",
    "                    try:\n",
    "                        return json.loads(candidate)\n",
    "                    except Exception:\n",
    "                        # continue scanning if not valid\n",
    "                        pass\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2757c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EACH MODEL FOR THEIR ANSWER TO THE USER PROMPT\n",
    "\n",
    "user_query = \"Explain how planets orbit around the sun\"\n",
    "\n",
    "rubric = \"\"\"Correctness & Accuracy (25 points) â€” Ensures claims are factually accurate and verifiable, addressing the most critical concern of hallucination-free responses. This is weighted highest because inaccurate information undermines all other qualities.\n",
    "\n",
    "Completeness (20 points) - Verifies the answer addresses all aspects of the query without significant omissions. This prevents shallow or partial responses that technically answer only part of the question.\n",
    "\n",
    "Clarity & Coherence (18 points) - Assesses whether the answer is well-organized with logical flow. Research shows that coherence and relevance are strong signals of problem-solving quality.\n",
    "\n",
    "Relevance (18 points) - Ensures all information pertains to the question, avoiding tangential content that confuses the issue. This maintains focus and efficiency.\n",
    "\n",
    "Conciseness (10 points) - Rewards efficiency by penalizing unnecessary verbosity or repetition while maintaining completeness. This balances against verbose but complete responses.\n",
    "\n",
    "Appropriateness for Context (9 points) â€” Checks whether tone, depth, and format match what the questioner likely needs. Technical questions require different treatment than conversational ones.\"\"\"\n",
    "\n",
    "# Extract model names from candidate_models\n",
    "model_names = [model[\"name\"] for model in candidate_models]\n",
    "result = await query_models(model_names, [user_query]*len(model_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72be76f",
   "metadata": {},
   "source": [
    "# PROMPT BASED SCORING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85fe80c",
   "metadata": {},
   "source": [
    "#### $n^2$ APPROACH\n",
    "Here, if we have $n$ models and model $M_1$ is evaluating model $M_2, M_3,..., M_n$, a single query will be sent for each evaluation i.e. we query $M_1$ with $M_2$'s answer, then query $M_1$ with $M_3$'s answer, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING THE QUERIES FOR EACH MODEL TO EVALUATE EACH OTHER\n",
    "\n",
    "\n",
    "scoring_query = lambda answer : f\"\"\"\\\n",
    "You are an expert evaluator for a large language model comparison tool. Your role is to provide an objective, rubric-based score for the candidate's response to a user's query.\n",
    "\n",
    "QUERY:\n",
    "{user_query}\n",
    "\n",
    "CANDIDATE RESPONSE:\n",
    "{answer}\n",
    "\n",
    "RUBRIC:\n",
    "{rubric}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Evaluate the Candidate Response on all rubric dimensions individually, strictly applying the rubric's defined score ranges and weightingsâ€”for example, Correctness & Accuracy is out of 25 points, Completeness 20 points, etc.\n",
    "\n",
    "If the Candidate Response contains any factual inaccuracies, assign the Correctness & Accuracy score corresponding to those errors as explicitly defined in the rubric, which could be as low as 0-4 out of 25 for fundamental factual errors. Do not inflate this score due to other qualities.\n",
    "\n",
    "Calculate the overall score as the weighted sum of all dimension scores, without subjective adjustment or rounding beyond rubric guidance.\n",
    "\n",
    "Your output must be ONLY a JSON object with:\n",
    "\n",
    "1. \"reasoning\": \"<One-sentence justification explicitly referencing rubric criteria and weights, including correctness importance>\",\n",
    "\n",
    "2.\"score\": <integer score from 0 to 100>\n",
    "\n",
    "Use your judgment to apply rubric weightings accurately, and remember that Correctness & Accuracy has the highest impact on the overall score.\n",
    "\"\"\"\n",
    "\n",
    "new_models_to_use = []\n",
    "new_queries_to_use = []\n",
    "models_being_evaluated = []\n",
    "for model1 in model_names:\n",
    "    for item in result:\n",
    "        model2, answer = item[\"model\"], item[\"response\"]\n",
    "        if model1 != model2:\n",
    "            new_models_to_use.append(model1)\n",
    "            new_queries_to_use.append(scoring_query(answer))\n",
    "            models_being_evaluated.append(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d740aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EACH MODEL TO EVALUATE EACH OTHER MODEL'S ANSWER\n",
    "\n",
    "scoring_results = await query_models(new_models_to_use, new_queries_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba748957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESPONSES & SCORES\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "i = 0\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        if model1 != model2:\n",
    "            print(f\"Judge Model: {model1}\\n\")\n",
    "            print(f\"Evaluated Model: {model2}\\n\")\n",
    "            print(f\"Evaluated Model's Answer:\")\n",
    "            print(scoring_results[i][\"response\"])\n",
    "            i += 1\n",
    "            print(\"--------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5899dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD RUBRIC FROM MODELS EVALUATIONS OF EACH OTHER\n",
    "\n",
    "scores_table = pd.DataFrame(\n",
    "    np.nan, \n",
    "    index=pd.Index(model_names, name=\"Judge Model (Row)\"),\n",
    "    columns=pd.Index(model_names, name=\"Evaluated Model (Column)\")\n",
    ")\n",
    "i = 0\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        if model1 != model2:\n",
    "            extracted_json = extract_outermost_json(scoring_results[i][\"response\"])\n",
    "            if \"score\" in extracted_json.keys():\n",
    "                score_value = extract_outermost_json(scoring_results[i][\"response\"])[\"score\"]\n",
    "            else:\n",
    "                score_value = np.nan\n",
    "            if not np.isnan(score_value):\n",
    "                scores_table.loc[model1, model2] = round(int(score_value), 2)\n",
    "            else:\n",
    "                scores_table.loc[model1, model2] = np.nan\n",
    "            i += 1\n",
    "display(scores_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c230ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINNING MODEL\n",
    "\n",
    "max_mean = -1\n",
    "max_models = []\n",
    "for model in model_names:\n",
    "    mean_score = scores_table[model].mean()\n",
    "    if mean_score > max_mean:\n",
    "        max_mean = mean_score\n",
    "        max_models = [model]\n",
    "    elif mean_score == max_mean:\n",
    "        max_models.append(model)\n",
    "\n",
    "if len(max_models) == 1:\n",
    "    max_model = max_models[0]\n",
    "    print(\"---------- WINNING RESPONSE ----------\")\n",
    "    print(max_model)\n",
    "    result_dict = {item[\"model\"]: item[\"response\"] for item in result}\n",
    "    print(result_dict[max_model])\n",
    "else:\n",
    "    print(\"Tie detected between the following models:\")\n",
    "    print(max_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687e175",
   "metadata": {},
   "source": [
    "#### $n$ APPROACH\n",
    "Here, if we have $n$ models and model $M_1$ is evaluating model $M_2, M_3,..., M_n$, a single query will be sent for all evaluations i.e. we query $M_1$ with $M_2, M_3,..., M_n$'s answers, then query $M_1, M_3,..., M_n$'s answer, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc51633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING THE QUERIES FOR EACH MODEL TO EVALUATE EACH OTHER\n",
    "\n",
    "def scoring_query(model):\n",
    "\n",
    "    answers = \"\"\n",
    "    for other_model in result:\n",
    "        if model != other_model[\"model\"]:\n",
    "            answers += f\"{other_model[\"model\"]} RESPONSE:\\n\" + other_model[\"response\"] + \"\\n\\n\"\n",
    "\n",
    "    return f\"\"\"\\\n",
    "You are an expert evaluator for a large language model comparison tool. Your role is to provide objective, rubric-based scores for the candidate's responses to a user's query.\n",
    "\n",
    "QUERY:\n",
    "{user_query}\n",
    "\n",
    "{answers}\n",
    "\n",
    "RUBRIC:\n",
    "{rubric}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Evaluate all the Candidates Responses on all rubric dimensions individually, strictly applying the rubric's defined score ranges and weightingsâ€”for example, Correctness & Accuracy is out of 25 points, Completeness 20 points, etc.\n",
    "\n",
    "If any of the Candidates Responses contain any factual inaccuracies, assign the Correctness & Accuracy score corresponding to those errors as explicitly defined in the rubric, which could be as low as 0-4 out of 25 for fundamental factual errors. Do not inflate this score due to other qualities.\n",
    "\n",
    "Calculate the overall score as the weighted sum of all dimension scores for each Candidate Response, without subjective adjustment or rounding beyond rubric guidance.\n",
    "\n",
    "Your output must be ONLY a JSON object with:\n",
    "\n",
    "1. \"model\": \"<full and exact name of the model as provided in this prompt>\"\n",
    "\n",
    "1.1. \"reasoning\": \"<One-sentence justification explicitly referencing rubric criteria and weights, including correctness importance>\",\n",
    "\n",
    "1.2. \"score\": <integer score from 0 to 100>\n",
    "\n",
    "E.g. {{\"<name of model1>\": {{\"reasoning\": \"<reasoning for model1>\", \"score\": \"<score for model1>\"}}, \"<name of model2>\": {{\"reasoning\": \"<reasoning for model2>\", \"score\": \"<score for model2>\"}}}}\n",
    "\n",
    "Here are the model names for reference: {model_names}\n",
    "\n",
    "Use your judgment to apply rubric weightings accurately, and remember that Correctness & Accuracy has the highest impact on the overall score.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "new_queries_to_use = []\n",
    "for model in model_names:\n",
    "    new_queries_to_use.append(scoring_query(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c91b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EACH MODEL TO EVALUATE EACH OTHER MODEL'S ANSWER\n",
    "\n",
    "scoring_results = await query_models(model_names, new_queries_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e1aa41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "High correctness (25/25) with factual accuracy, moderately complete (17/20) missing barycenter/relativity, clear structure (17/18), fully relevant (18/18), concise (9/10), and context-appropriate (8/9).\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "Perfect correctness (25/25), comprehensive completeness (20/20) covering advanced concepts, exceptional clarity/organization (18/18), full relevance (18/18), efficient conciseness (9/10), and exemplary context adaptation (9/9).\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "The response from TNG: DeepSeek R1T2 Chimera (free) is highly accurate, complete, and clear, addressing all aspects of the query with a logical flow, earning high scores across all dimensions, particularly in Correctness & Accuracy, which is weighted the highest at 25 points, thus significantly contributing to its overall score.\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "The response from OpenAI: gpt-oss-20b (free) is also accurate and complete but slightly less clear and concise compared to TNG: DeepSeek R1T2 Chimera (free), with a more simplistic explanation that still covers the necessary points, resulting in a high score but with deductions for clarity and conciseness, which are important but lesser-weighted criteria.\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "The response scores highly on Correctness (25/25) for accurate physics; Completeness (20/20) for covering all key principles; Clarity & Coherence (18/18) with clear structure; Relevance (18/18) for focused content; Conciseness (8/10) for moderate length; Appropriateness (9/9) for suitable tone, yielding an overall 98/100.\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "The answer achieves good Correctness (22/25) despite a minor terminology slip; Completeness (15/20) misses concepts like inertia; Clarity & Coherence (16/18) is coherent but less detailed; Relevance (18/18) fully relevant; Conciseness (9/10) is concise; Appropriateness (9/9) fits the query, totaling 89/100.\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# RESPONSES & SCORES\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "for i in range(len(model_names)):\n",
    "    extracted_json = extract_outermost_json(scoring_results[i][\"response\"])\n",
    "    for evaluated_model in extracted_json.keys():\n",
    "        print(f\"Judge Model: {model_names[i]}\\n\")\n",
    "        print(f\"Evaluated Model: {evaluated_model}\\n\")\n",
    "        print(f\"Evaluated Model's Answer:\")\n",
    "        print(extracted_json[evaluated_model][\"reasoning\"])\n",
    "        print(\"--------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d664300f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Evaluated Model (Column)</th>\n",
       "      <th>TNG: DeepSeek R1T2 Chimera (free)</th>\n",
       "      <th>Meta: Llama 3.3 70B Instruct (free)</th>\n",
       "      <th>OpenAI: gpt-oss-20b (free)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge Model (Row)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TNG: DeepSeek R1T2 Chimera (free)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>94.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta: Llama 3.3 70B Instruct (free)</th>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenAI: gpt-oss-20b (free)</th>\n",
       "      <td>98.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Evaluated Model (Column)             TNG: DeepSeek R1T2 Chimera (free)  \\\n",
       "Judge Model (Row)                                                        \n",
       "TNG: DeepSeek R1T2 Chimera (free)                                  NaN   \n",
       "Meta: Llama 3.3 70B Instruct (free)                               95.0   \n",
       "OpenAI: gpt-oss-20b (free)                                        98.0   \n",
       "\n",
       "Evaluated Model (Column)             Meta: Llama 3.3 70B Instruct (free)  \\\n",
       "Judge Model (Row)                                                          \n",
       "TNG: DeepSeek R1T2 Chimera (free)                                   94.0   \n",
       "Meta: Llama 3.3 70B Instruct (free)                                  NaN   \n",
       "OpenAI: gpt-oss-20b (free)                                          89.0   \n",
       "\n",
       "Evaluated Model (Column)             OpenAI: gpt-oss-20b (free)  \n",
       "Judge Model (Row)                                                \n",
       "TNG: DeepSeek R1T2 Chimera (free)                          99.0  \n",
       "Meta: Llama 3.3 70B Instruct (free)                        88.0  \n",
       "OpenAI: gpt-oss-20b (free)                                  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BUILD RUBRIC FROM MODELS EVALUATIONS OF EACH OTHER\n",
    "\n",
    "scores_table = pd.DataFrame(\n",
    "    np.nan, \n",
    "    index=pd.Index(model_names, name=\"Judge Model (Row)\"),\n",
    "    columns=pd.Index(model_names, name=\"Evaluated Model (Column)\")\n",
    ")\n",
    "i = 0\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        if model1 != model2:\n",
    "            extracted_json = extract_outermost_json(scoring_results[i][\"response\"])[model2]\n",
    "            if \"score\" in extracted_json.keys():\n",
    "                score_value = extracted_json[\"score\"]\n",
    "            else:\n",
    "                score_value = np.nan\n",
    "            if not np.isnan(score_value):\n",
    "                scores_table.loc[model1, model2] = round(int(score_value), 2)\n",
    "            else:\n",
    "                scores_table.loc[model1, model2] = np.nan\n",
    "    i += 1\n",
    "display(scores_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd215a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- WINNING RESPONSE ----------\n",
      "TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Planets orbit the Sun due to the interplay between **gravity** and **inertia**, governed by the laws of physics discovered by scientists like Kepler and Newton. Hereâ€™s a step-by-step explanation:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Gravity: The Sunâ€™s Pull**\n",
      "   - The Sunâ€™s immense mass creates a **gravitational field** that pulls planets toward it.\n",
      "   - **Newtonâ€™s law of universal gravitation** states that every object with mass attracts every other object. The force depends on:\n",
      "     - The masses of the Sun and planet.\n",
      "     - The distance between them (weaker as distance increases).\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Inertia: The Planetâ€™s Motion**\n",
      "   - Planets are not stationaryâ€”they have **inertia** (a tendency to move in a straight line at constant speed).\n",
      "   - When the Sunâ€™s gravity pulls a planet, it bends this straight-line motion into a curved path (orbit).\n",
      "   - Think of swinging a ball on a string: the stringâ€™s pull (gravity) keeps the ball (planet) circling instead of flying away.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Balancing Act: Stable Orbits**\n",
      "   - For a stable orbit, the planetâ€™s **forward motion** (tangential velocity) and the Sunâ€™s **gravitational pull** must balance perfectly.\n",
      "   - **Too slow**: The planet spirals into the Sun.\n",
      "   - **Too fast**: It escapes the Sunâ€™s gravity (becomes interstellar).\n",
      "   - **Just right**: It follows a continuous elliptical path around the Sun.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Shape of Orbits: Keplerâ€™s Laws**\n",
      "   - **Keplerâ€™s First Law**: Orbits are **elliptical** (oval-shaped), not perfect circles. The Sun sits at one **focus** of the ellipse.\n",
      "     - *Perihelion*: Closest point to the Sun.\n",
      "     - *Aphelion*: Farthest point.\n",
      "   - **Keplerâ€™s Second Law**: Planets speed up when closer to the Sun and slow down when farther away (sweeping equal areas in equal times).\n",
      "   - **Keplerâ€™s Third Law**: The time a planet takes to orbit the Sun (orbital period) depends on its average distance from the Sun.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. **Einsteinâ€™s Refinement: General Relativity**\n",
      "   - Newtonâ€™s laws work for most cases, but Einstein showed gravity arises from the **curvature of spacetime** by mass.\n",
      "   - Planets follow the â€œstraightest possible pathâ€ (geodesic) in curved spacetime around the Sun, explaining tiny quirks like Mercuryâ€™s orbit.\n",
      "\n",
      "---\n",
      "\n",
      "### Why Donâ€™t Planets Crash Into Each Other?\n",
      "- Orbits are **predictable** due to gravity and inertia.\n",
      "- Planets formed in a disk of gas/dust (protoplanetary disk), so their orbits are mostly **coplanar** (aligned) and spaced far apart.\n",
      "\n",
      "---\n",
      "\n",
      "### Visualization\n",
      "- Imagine rolling a marble around the edge of a funnel: its speed keeps it from falling inward, just like gravity and inertia keep planets in orbit!\n",
      "\n",
      "**In short**: Planets orbit the Sun because their forward motion balances the Sunâ€™s gravitational pull, resulting in stable, elliptical paths described by Keplerâ€™s and Newtonâ€™s laws. ðŸŒŒ\n"
     ]
    }
   ],
   "source": [
    "# WINNING MODEL\n",
    "\n",
    "max_mean = -1\n",
    "max_models = []\n",
    "for model in model_names:\n",
    "    mean_score = scores_table[model].mean()\n",
    "    if mean_score > max_mean:\n",
    "        max_mean = mean_score\n",
    "        max_models = [model]\n",
    "    elif mean_score == max_mean:\n",
    "        max_models.append(model)\n",
    "\n",
    "if len(max_models) == 1:\n",
    "    max_model = max_models[0]\n",
    "    print(\"---------- WINNING RESPONSE ----------\")\n",
    "    print(max_model)\n",
    "    result_dict = {item[\"model\"]: item[\"response\"] for item in result}\n",
    "    print(result_dict[max_model])\n",
    "else:\n",
    "    print(\"Tie detected between the following models:\")\n",
    "    print(max_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbb0ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
