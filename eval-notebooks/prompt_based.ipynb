{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a23bfbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import asyncio\n",
    "import re\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "74acda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in .env file or environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d9189a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=OPENROUTER_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "43b00259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL AVAILABLE MODELS FOR TESTING\n",
    "# *to add models, use the screen name for the model from OpenRouter as the key and\n",
    "# the actual name used in the api as the value\n",
    "\n",
    "candidate_models = [\n",
    "    # PAID\n",
    "    # {\"name\": \"Anthropic: Claude Sonnet 4.5\", \"openrouter\": \"anthropic/claude-sonnet-4.5\"},\n",
    "    # {\"name\": \"OpenAI: GPT-4o\", \"openrouter\": \"openai/gpt-4o\"},\n",
    "    # {\"name\": \"OpenAI: GPT-4o Mini\", \"openrouter\": \"openai/gpt-4o-mini\"}\n",
    "    # FREE\n",
    "    {\"name\": \"MiniMax: MiniMax M2 (free)\", \"openrouter\": \"minimax/minimax-m2:free\"},\n",
    "    {\"name\": \"TNG: DeepSeek R1T2 Chimera (free)\", \"openrouter\": \"tngtech/deepseek-r1t2-chimera:free\"},\n",
    "    {\"name\": \"Meta: Llama 3.3 70B Instruct (free)\", \"openrouter\": \"meta-llama/llama-3.3-70b-instruct:free\"},\n",
    "    {\"name\": \"OpenAI: gpt-oss-20b (free)\", \"openrouter\": \"openai/gpt-oss-20b:free\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "72eeda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY FUNCTIONS\n",
    "\n",
    "async def query_model(model_name: str, query: str, role=\"user\"):\n",
    "    \"\"\"\n",
    "    Queries a single model using the models in 'candidate_models'\n",
    "    Args:\n",
    "        model_name: The name of the model (from candidate_models \"name\" field)\n",
    "    Returns: dict with keys 'model' and 'response'\n",
    "    \"\"\"\n",
    "    model_dict = next((m for m in candidate_models if m[\"name\"] == model_name), None)\n",
    "    if model_dict is None:\n",
    "        return {\"model\": model_name, \"response\": f\"Error: Model '{model_name}' not found in candidate_models\"}\n",
    "    \n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=model_dict[\"openrouter\"],\n",
    "            messages=[{\"role\" : role, \"content\" : query}],\n",
    "            temperature=1\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        return {\"model\": model_name, \"response\": content}\n",
    "    except Exception as e:\n",
    "        return {\"model\": model_name, \"response\": str(e)}\n",
    "\n",
    "async def query_models(model_names: list[str], queries: list[str], role=\"user\"):\n",
    "    \"\"\"\n",
    "    Queries multiple models asynchronously\n",
    "    Args:\n",
    "        model_names: List of model names (from candidate_models \"name\" field)\n",
    "    \"\"\"\n",
    "    coroutines = [query_model(model_names[i], queries[i], role=role) for i in range(len(model_names))]\n",
    "    results = await asyncio.gather(*coroutines)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fe820",
   "metadata": {},
   "source": [
    "# PROMPT BASED SCORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "2757c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EACH MODEL FOR THEIR ANSWER TO THE USER PROMPT\n",
    "\n",
    "user_query = \"Explain how planets orbit around the sun\"\n",
    "\n",
    "rubric = \"\"\"Correctness & Accuracy (25 points) — Ensures claims are factually accurate and verifiable, addressing the most critical concern of hallucination-free responses. This is weighted highest because inaccurate information undermines all other qualities.\n",
    "\n",
    "Completeness (20 points) - Verifies the answer addresses all aspects of the query without significant omissions. This prevents shallow or partial responses that technically answer only part of the question.\n",
    "\n",
    "Clarity & Coherence (18 points) - Assesses whether the answer is well-organized with logical flow. Research shows that coherence and relevance are strong signals of problem-solving quality.\n",
    "\n",
    "Relevance (18 points) - Ensures all information pertains to the question, avoiding tangential content that confuses the issue. This maintains focus and efficiency.\n",
    "\n",
    "Conciseness (10 points) - Rewards efficiency by penalizing unnecessary verbosity or repetition while maintaining completeness. This balances against verbose but complete responses.\n",
    "\n",
    "Appropriateness for Context (9 points) — Checks whether tone, depth, and format match what the questioner likely needs. Technical questions require different treatment than conversational ones.\"\"\"\n",
    "\n",
    "# Extract model names from candidate_models\n",
    "model_names = [model[\"name\"] for model in candidate_models]\n",
    "result = await query_models(model_names, [user_query]*len(model_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85fe80c",
   "metadata": {},
   "source": [
    "#### $n^2$ APPROACH\n",
    "Here, if we have $n$ models and model $M_1$ is evaluating model $M_2, M_3,..., M_n$, a single query will be sent for each evaluation i.e. we query $M_1$ with $M_2$'s answer, then query $M_1$ with $M_3$'s answer, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "522a831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING THE QUERIES FOR EACH MODEL TO EVALUATE EACH OTHER\n",
    "\n",
    "\n",
    "scoring_query = lambda answer : f\"\"\"\\\n",
    "You are an expert evaluator for a large language model comparison tool. Your role is to provide an objective, rubric-based score for the candidate's response to a user's query.\n",
    "\n",
    "QUERY:\n",
    "{user_query}\n",
    "\n",
    "CANDIDATE RESPONSE:\n",
    "{answer}\n",
    "\n",
    "RUBRIC:\n",
    "{rubric}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Evaluate the Candidate Response on all rubric dimensions individually, strictly applying the rubric's defined score ranges and weightings—for example, Correctness & Accuracy is out of 25 points, Completeness 20 points, etc.\n",
    "\n",
    "If the Candidate Response contains any factual inaccuracies, assign the Correctness & Accuracy score corresponding to those errors as explicitly defined in the rubric, which could be as low as 0-4 out of 25 for fundamental factual errors. Do not inflate this score due to other qualities.\n",
    "\n",
    "Calculate the overall score as the weighted sum of all dimension scores, without subjective adjustment or rounding beyond rubric guidance.\n",
    "\n",
    "Your output must be ONLY a JSON object with:\n",
    "\n",
    "1. \"reasoning\": \"<One-sentence justification explicitly referencing rubric criteria and weights, including correctness importance>\",\n",
    "\n",
    "2.\"score\": <integer score from 0 to 100>\n",
    "\n",
    "Use your judgment to apply rubric weightings accurately, and remember that Correctness & Accuracy has the highest impact on the overall score.\n",
    "\"\"\"\n",
    "\n",
    "new_models_to_use = []\n",
    "new_queries_to_use = []\n",
    "models_being_evaluated = []\n",
    "for model1 in model_names:\n",
    "    for item in result:\n",
    "        model2, answer = item[\"model\"], item[\"response\"]\n",
    "        if model1 != model2:\n",
    "            new_models_to_use.append(model1)\n",
    "            new_queries_to_use.append(scoring_query(answer))\n",
    "            models_being_evaluated.append(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "1d740aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EACH MODEL TO EVALUATE EACH OTHER MODEL'S ANSWER\n",
    "\n",
    "scoring_results = await query_models(new_models_to_use, new_queries_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ba748957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: MiniMax: MiniMax M2 (free)\n",
      "\n",
      "Evaluated Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "{\"reasoning\": \"The response demonstrates high factual accuracy regarding orbital mechanics, comprehensively covers the key forces and principles, maintains clear organization and logical flow, and stays fully relevant to the query while using an appropriate educational tone and format.\", \"score\": 93}\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: MiniMax: MiniMax M2 (free)\n",
      "\n",
      "Evaluated Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"The response contains fundamental factual errors in its explanation of orbital mechanics, particularly the misleading description of centrifugal force versus centripetal force and inertia, which significantly undermines scientific accuracy despite good organization and completeness.\",\n",
      "  \"score\": 52\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: MiniMax: MiniMax M2 (free)\n",
      "\n",
      "Evaluated Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "{\n",
      "  \"reasoning\": \"The response achieves maximum scores for correctness and accuracy (25/25) due to its scientifically sound content, full completeness (20/20) by covering all aspects of planetary orbits, and high marks in clarity, relevance, conciseness, and appropriateness, with correctness weighted highest to ensure no hallucinations or errors.\",\n",
      "  \"score\": 100\n",
      "}\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model: MiniMax: MiniMax M2 (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"The response is factually accurate (Correctness & Accuracy: 25/25), fully addresses all aspects of the query (Completeness: 20/20), is well-organized (Clarity & Coherence: 18/18), remains strictly relevant (Relevance: 18/18), concise (Conciseness: 10/10), and matches scientific context appropriately (Appropriateness: 9/9), resulting in a perfect weighted score.\",\n",
      "  \"score\": 100\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "\n",
      "{\n",
      "  \"reasoning\": \"While generally accurate, the response incorrectly presents centrifugal force as a real force rather than a fictitious effect, qualifying as a fundamental factual error. However, core explanations of gravity, velocity, and Kepler's laws remain correct. Scores adjusted per rubric weights with Correctness & Accuracy's high priority (moderate error range: 15-19/25). Other dimensions scored highly for completeness (18/20), clarity (17/18), relevance (17/18), and balanced conciseness (8/10) and appropriateness (8/9).\",\n",
      "  \"score\": 83\n",
      "}\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"The response scores maximum points in all rubric categories: it is factually accurate (25/25), complete addressing all key aspects (20/20), clearly structured (18/18), fully relevant (18/18), concise despite technical depth (10/10), and appropriately technical for the query (9/9), with correctness prioritized as the highest-weighted criterion.\",\n",
      "  \"score\": 100\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model: MiniMax: MiniMax M2 (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"The candidate response scores highly across all dimensions, particularly in Correctness & Accuracy, which is weighted at 25 points and is crucial for the overall score, as it provides a detailed, factually accurate, and well-organized explanation of how planets orbit the Sun, addressing all aspects of the query without significant omissions and maintaining relevance and coherence throughout, thus achieving a high overall score.\",\n",
      "  \"score\": 96\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"The Candidate Response scores highly across all dimensions, particularly in Correctness & Accuracy, which is weighted at 25 points and is crucial for a hallucination-free response, with the response accurately describing planetary orbits around the Sun through the balance of gravity and inertia, thus earning a high overall score due to its strong performance in this and other areas such as Completeness, Clarity & Coherence, Relevance, Conciseness, and Appropriateness for Context.\",\n",
      "  \"score\": 96\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"The response is evaluated based on the provided rubric, with Correctness & Accuracy being the most critical factor, and considering the high quality, accuracy, and completeness of the information provided, along with its clarity, relevance, conciseness, and appropriateness for the context, resulting in a score that reflects its excellence across all weighted dimensions.\",\n",
      "  \"score\": 98\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model: MiniMax: MiniMax M2 (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "{\"reasoning\":\"Score reflects high factual accuracy (23/25), thorough coverage of orbital dynamics (18/20), clear structured presentation (16/18), relevance to the query with no digressions (17/18), and moderate conciseness with some extra detail (7/10), all weighted per rubric for an overall 89/100.\",\"score\":89}\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "{\"reasoning\":\"The response accurately describes gravitational and inertial dynamics, fully covers the requested explanation, is logically organized, relevant, concise, and appropriately styled—meeting all weighted rubric criteria, especially the highest‑weighted Correctness & Accuracy.\",\"score\":100}\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "{\"reasoning\":\"The answer is largely factually correct and thorough, with only a minor mis‑labeling of Kepler’s third law, earning high Correctness & Accuracy (23/25). It covers all required concepts—gravity, orbital velocity, balance, ellipses, Kepler’s laws—so Completeness is strong (18/20). The structure and flow are logical, supporting Clarity & Coherence (16/18). All included information remains on topic for Relevance (17/18). The response is somewhat verbose but still concise enough (7/10). The enthusiastic tone is appropriate for a general explanation, scoring Appropriateness (8/9). Overall weighted score sums to 89/100.\",\"score\":89}\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# RESPONSES & SCORES\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "i = 0\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        if model1 != model2:\n",
    "            print(f\"Judge Model: {model1}\\n\")\n",
    "            print(f\"Evaluated Model: {model2}\\n\")\n",
    "            print(f\"Evaluated Model's Answer:\")\n",
    "            print(scoring_results[i][\"response\"])\n",
    "            i += 1\n",
    "            print(\"--------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8e469108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score_from_response(response_text: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Extract score from LLM response, handling various formats.\n",
    "    \n",
    "    Handles:\n",
    "    - JSON wrapped in markdown code blocks (```json ... ```)\n",
    "    - Plain JSON objects\n",
    "    - Returns None if parsing fails\n",
    "    \"\"\"\n",
    "    # First, try to extract JSON from markdown code blocks\n",
    "    json_match = re.search(r'```(?:json)?\\s*(\\{.*?\\})\\s*```', response_text, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(1)\n",
    "    else:\n",
    "        # Try to find any JSON object in the text\n",
    "        json_match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(0)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        parsed = json.loads(json_str)\n",
    "        return parsed.get(\"score\")\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "d5899dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Evaluated Model (Column)</th>\n",
       "      <th>MiniMax: MiniMax M2 (free)</th>\n",
       "      <th>TNG: DeepSeek R1T2 Chimera (free)</th>\n",
       "      <th>Meta: Llama 3.3 70B Instruct (free)</th>\n",
       "      <th>OpenAI: gpt-oss-20b (free)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge Model (Row)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MiniMax: MiniMax M2 (free)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNG: DeepSeek R1T2 Chimera (free)</th>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta: Llama 3.3 70B Instruct (free)</th>\n",
       "      <td>96.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenAI: gpt-oss-20b (free)</th>\n",
       "      <td>89.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Evaluated Model (Column)             MiniMax: MiniMax M2 (free)  \\\n",
       "Judge Model (Row)                                                 \n",
       "MiniMax: MiniMax M2 (free)                                  NaN   \n",
       "TNG: DeepSeek R1T2 Chimera (free)                         100.0   \n",
       "Meta: Llama 3.3 70B Instruct (free)                        96.0   \n",
       "OpenAI: gpt-oss-20b (free)                                 89.0   \n",
       "\n",
       "Evaluated Model (Column)             TNG: DeepSeek R1T2 Chimera (free)  \\\n",
       "Judge Model (Row)                                                        \n",
       "MiniMax: MiniMax M2 (free)                                        93.0   \n",
       "TNG: DeepSeek R1T2 Chimera (free)                                  NaN   \n",
       "Meta: Llama 3.3 70B Instruct (free)                               96.0   \n",
       "OpenAI: gpt-oss-20b (free)                                       100.0   \n",
       "\n",
       "Evaluated Model (Column)             Meta: Llama 3.3 70B Instruct (free)  \\\n",
       "Judge Model (Row)                                                          \n",
       "MiniMax: MiniMax M2 (free)                                          52.0   \n",
       "TNG: DeepSeek R1T2 Chimera (free)                                   83.0   \n",
       "Meta: Llama 3.3 70B Instruct (free)                                  NaN   \n",
       "OpenAI: gpt-oss-20b (free)                                          89.0   \n",
       "\n",
       "Evaluated Model (Column)             OpenAI: gpt-oss-20b (free)  \n",
       "Judge Model (Row)                                                \n",
       "MiniMax: MiniMax M2 (free)                                100.0  \n",
       "TNG: DeepSeek R1T2 Chimera (free)                         100.0  \n",
       "Meta: Llama 3.3 70B Instruct (free)                        98.0  \n",
       "OpenAI: gpt-oss-20b (free)                                  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BUILD RUBRIC FROM MODELS EVALUATIONS OF EACH OTHER\n",
    "\n",
    "rubric = pd.DataFrame(\n",
    "    np.nan, \n",
    "    index=pd.Index(model_names, name=\"Judge Model (Row)\"),\n",
    "    columns=pd.Index(model_names, name=\"Evaluated Model (Column)\")\n",
    ")\n",
    "i = 0\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        if model1 != model2:\n",
    "            score_value = extract_score_from_response(scoring_results[i][\"response\"])\n",
    "            if score_value is not None:\n",
    "                rubric.loc[model1, model2] = round(int(score_value), 2)\n",
    "            else:\n",
    "                rubric.loc[model1, model2] = np.nan\n",
    "            i += 1\n",
    "display(rubric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "0c230ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- WINNING RESPONSE ----------\n",
      "OpenAI: gpt-oss-20b (free)\n",
      "## Why Planets Circle the Sun\n",
      "\n",
      "All of the bodies in our Solar System are held together by **gravity**—a universal attraction that pulls every mass toward every other mass.  \n",
      "Because the Sun contains more than 99 % of the system’s mass, its gravitational pull dominates the motion of the eight (plus dwarf planets and comets) that revolve around it.  \n",
      "\n",
      "### 1. Newton’s Universal Law of Gravitation\n",
      "\n",
      "\\[\n",
      "F=\\frac{G\\,M_\\odot\\,m}{r^{2}}\n",
      "\\]\n",
      "\n",
      "* \\(F\\) gravitational force between the Sun and a planet (or any object).  \n",
      "* \\(M_\\odot\\) mass of the Sun.  \n",
      "* \\(m\\) mass of the orbiting body.  \n",
      "* \\(r\\) distance between the centers of the two masses.  \n",
      "* \\(G\\) Gravitational constant \\(\\;≈ 6.67\\times 10^{-11}\\; \\text{N·m²·kg⁻²}\\).\n",
      "\n",
      "Because the force falls with the **square** of distance, the Sun’s pull weakens as a planet moves farther from it.\n",
      "\n",
      "### 2. Two Competing Motions: Tangential Speed vs. Radial Pull\n",
      "\n",
      "When a planet is moving around the Sun it has two aspects to its motion:\n",
      "\n",
      "| Tangential (side‑to‑side) | Radial (toward‑or‑away from Sun) |\n",
      "|---------------------------|-----------------------------------|\n",
      "| Gives the planet angular momentum. | Causes the planet to fall toward the Sun. |\n",
      "\n",
      "If the tangential speed is too low, gravity pulls the planet straight in (a collision).  \n",
      "If it’s too high, the planet will spiral away.  \n",
      "Newton’s equations show that there is a **stable speed** for a planet at a given distance: the balance between the **centripetal requirement** and the gravitational pull.  \n",
      "\n",
      "For a **circular** orbit that speed is\n",
      "\n",
      "\\[\n",
      "v = \\sqrt{\\frac{G M_\\odot}{r}}\n",
      "\\]\n",
      "\n",
      "but real orbits are not perfect circles—they are **ellipses**.\n",
      "\n",
      "### 3. Kepler’s Three Laws (empirical law of orbits)\n",
      "\n",
      "1. **Elliptical paths:**  \n",
      "   Each planet moves in an ellipse with the Sun at one focus.  \n",
      "   An ellipse is defined by its semi‑major axis \\(a\\) and eccentricity \\(e\\).  \n",
      "   For the Sun, the Sun is exactly at one focus because it’s much, much heavier than any planet.\n",
      "\n",
      "2. **Equal areas in equal times:**  \n",
      "   The line connecting the Sun to the planet sweeps out equal areas during equal intervals of time.  \n",
      "   This is equivalent to conservation of angular momentum (\\(L = m v r = \\text{constant}\\)).  \n",
      "   Because the planet is closer to the Sun at perihelion, it moves faster there.\n",
      "\n",
      "3. **Harmonic law (period‑size relationship):**  \n",
      "   \\[\n",
      "   T^2 \\propto a^3\n",
      "   \\]\n",
      "   The square of an orbit’s period \\(T\\) (time to make one revolution) is proportional to the cube of its semi‑major axis.  \n",
      "   (More formally: \\(T^2 = \\frac{4\\pi^2}{G M_\\odot}\\, a^3\\).)\n",
      "\n",
      "Kepler’s laws were derived from planetary observations (by Tycho Brahe, then Kepler) before we knew why they were true. Newton’s gravitation later explained them.\n",
      "\n",
      "### 4. How the Orbital Shape Arises\n",
      "\n",
      "Imagine throwing a ball on a very flat surface—if you throw it “just right” (proper tangential speed), it will keep looping around because its forward motion balances gravity pulling it back.  \n",
      "If you throw it slightly down, it will arc downward; throw it too high, and it will go out to a great distance and then fall back.\n",
      "\n",
      "The **shape** of the orbit depends on the ratio of the planet’s speed to this critical speed:\n",
      "\n",
      "* **Equal to critical speed **→** perfect circle.  \n",
      "* **Slightly more** → **ellipse** with perihelion (closest point) and aphelion (farthest point).  \n",
      "* Because the Sun is at one focus, the planet spends more time near aphelion than perihelion—Kepler’s second law.\n",
      "\n",
      "The **eccentricity** \\(e\\) (from 0 for a circle to 1 for a parabola) is set by the initial conditions (planet’s starting position, speed, and direction).\n",
      "\n",
      "### 5. Stability of Planetary Orbits\n",
      "\n",
      "Planets do not orbit in perfect ellipses forever.  \n",
      "Their paths are influenced by the small pull of other planets, the Sun’s oblateness, relativistic corrections, and, on larger scales, the expansion of the Universe (the effect is negligible on Solar‑system scales).  \n",
      "\n",
      "Despite these perturbations, angular momentum and energy are largely conserved, so orbits are **very stable** over billions of years.\n",
      "\n",
      "**For example**:  \n",
      "- Mars’ eccentricity has waxed and waned over millions of years, leading to slightly warmer or cooler climates.  \n",
      "- Mercury’s perihelion has a tiny precession (shift over time) explained by general relativity (≈43 arcseconds per century).\n",
      "\n",
      "### 6. The Solar System in One Diagram\n",
      "\n",
      "```\n",
      "          farthest point\n",
      "         o-------------\\\n",
      "        /                \\ \n",
      "   Sun @o                   o\n",
      "      \\   <-- orbital path   /\n",
      "       \\                    /\n",
      "        \\   close point     /\n",
      "         \\----------------/\n",
      "             planet\n",
      "```\n",
      "\n",
      "* The Sun (big dot) sits at one focus of the ellipse (shaped \"oval\").  \n",
      "* The planet (little dot) moves along the path, speeding up near the Sun (close point) and slowing down far away (farthest point).  \n",
      "* The path traces itself so that the area between the Sun and the planetary orbit, swept out over a given time, is always the same.\n",
      "\n",
      "---\n",
      "\n",
      "## Recap\n",
      "\n",
      "1. **Gravity pulls** every mass toward every other mass.  \n",
      "2. **Balance the pull** and the planet’s sideways motion → stable orbit.  \n",
      "3. **Kepler’s laws** describe the geometry and timing of those orbits: ellipses, equal areas, period‑size relation.  \n",
      "4. **Newton’s gravitation** gives the underlying formula that makes these laws true.  \n",
      "5. The Sun’s dominating mass keeps the planets on **stable, long‑lasting elliptic paths** that have governed life on Earth for billions of years.\n"
     ]
    }
   ],
   "source": [
    "# WINNING MODEL\n",
    "\n",
    "max_mean = -1\n",
    "max_models = []\n",
    "for model in model_names:\n",
    "    mean_score = rubric[model].mean()\n",
    "    if mean_score > max_mean:\n",
    "        max_mean = mean_score\n",
    "        max_models = [model]\n",
    "    elif mean_score == max_mean:\n",
    "        max_models.append(model)\n",
    "\n",
    "if len(max_models) == 1:\n",
    "    max_model = max_models[0]\n",
    "    print(\"---------- WINNING RESPONSE ----------\")\n",
    "    print(max_model)\n",
    "    result_dict = {item[\"model\"]: item[\"response\"] for item in result}\n",
    "    print(result_dict[max_model])\n",
    "else:\n",
    "    print(\"Tie detected between the following models:\")\n",
    "    print(max_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687e175",
   "metadata": {},
   "source": [
    "#### $n$ APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1bc51633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
