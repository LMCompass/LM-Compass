{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23bfbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import asyncio\n",
    "import re\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74acda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in .env file or environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9189a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=OPENROUTER_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b00259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL AVAILABLE MODELS FOR TESTING\n",
    "# *to add models, use the screen name for the model from OpenRouter as the key and\n",
    "# the actual name used in the api as the value\n",
    "# *comment out models that you don't want to be used\n",
    "\n",
    "candidate_models = [\n",
    "    # PAID\n",
    "    # {\"name\": \"Anthropic: Claude Sonnet 4.5\", \"openrouter\": \"anthropic/claude-sonnet-4.5\"},\n",
    "    # {\"name\": \"OpenAI: GPT-4o\", \"openrouter\": \"openai/gpt-4o\"},\n",
    "    # {\"name\": \"OpenAI: GPT-4o Mini\", \"openrouter\": \"openai/gpt-4o-mini\"}\n",
    "    # FREE\n",
    "    # {\"name\": \"MiniMax: MiniMax M2 (free)\", \"openrouter\": \"minimax/minimax-m2:free\"}, # Not working for me (OWEN)\n",
    "    {\"name\": \"TNG: DeepSeek R1T2 Chimera (free)\", \"openrouter\": \"tngtech/deepseek-r1t2-chimera:free\"},\n",
    "    {\"name\": \"Meta: Llama 3.3 70B Instruct (free)\", \"openrouter\": \"meta-llama/llama-3.3-70b-instruct:free\"},\n",
    "    {\"name\": \"OpenAI: gpt-oss-20b (free)\", \"openrouter\": \"openai/gpt-oss-20b:free\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72eeda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY FUNCTIONS\n",
    "\n",
    "async def query_model(model_name: str, query: str, role=\"user\"):\n",
    "    \"\"\"\n",
    "    Queries a single model using the models in 'candidate_models'\n",
    "    Args:\n",
    "        model_name: The name of the model (from candidate_models \"name\" field)\n",
    "    Returns: dict with keys 'model' and 'response'\n",
    "    \"\"\"\n",
    "    model_dict = next((m for m in candidate_models if m[\"name\"] == model_name), None)\n",
    "    if model_dict is None:\n",
    "        return {\"model\": model_name, \"response\": f\"Error: Model '{model_name}' not found in candidate_models\"}\n",
    "    \n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=model_dict[\"openrouter\"],\n",
    "            messages=[{\"role\" : role, \"content\" : query}],\n",
    "            temperature=1\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        return {\"model\": model_name, \"response\": content}\n",
    "    except Exception as e:\n",
    "        return {\"model\": model_name, \"response\": str(e)}\n",
    "\n",
    "async def query_models(model_names: list[str], queries: list[str], role=\"user\"):\n",
    "    \"\"\"\n",
    "    Queries multiple models asynchronously\n",
    "    Args:\n",
    "        model_names: List of model names (from candidate_models \"name\" field)\n",
    "    \"\"\"\n",
    "    coroutines = [query_model(model_names[i], queries[i], role=role) for i in range(len(model_names))]\n",
    "    results = await asyncio.gather(*coroutines)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ae6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON PARSER\n",
    "\n",
    "def extract_outermost_json(text):\n",
    "    \"\"\"\n",
    "    Extracts the outermost JSON object from arbitrary text.\n",
    "    Returns the parsed JSON (dict/list) or raises ValueError if no valid JSON found.\n",
    "    \"\"\"\n",
    "\n",
    "    start = None\n",
    "    depth = 0\n",
    "    in_string = False\n",
    "    escape = False\n",
    "\n",
    "    for i, ch in enumerate(text):\n",
    "        if escape:\n",
    "            escape = False\n",
    "            continue\n",
    "\n",
    "        if ch == \"\\\\\":\n",
    "            escape = True\n",
    "            continue\n",
    "\n",
    "        if ch == '\"':\n",
    "            in_string = not in_string\n",
    "            continue\n",
    "\n",
    "        if not in_string:\n",
    "            if ch == \"{\":\n",
    "                if depth == 0:\n",
    "                    start = i\n",
    "                depth += 1\n",
    "            elif ch == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0 and start is not None:\n",
    "                    candidate = text[start:i+1]\n",
    "                    try:\n",
    "                        return json.loads(candidate)\n",
    "                    except Exception:\n",
    "                        # continue scanning if not valid\n",
    "                        pass\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2757c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EACH MODEL FOR THEIR ANSWER TO THE USER PROMPT\n",
    "\n",
    "user_query = \"Explain how planets orbit around the sun\"\n",
    "\n",
    "rubric = \"\"\"Correctness & Accuracy (25 points) â€” Ensures claims are factually accurate and verifiable, addressing the most critical concern of hallucination-free responses. This is weighted highest because inaccurate information undermines all other qualities.\n",
    "\n",
    "Completeness (20 points) - Verifies the answer addresses all aspects of the query without significant omissions. This prevents shallow or partial responses that technically answer only part of the question.\n",
    "\n",
    "Clarity & Coherence (18 points) - Assesses whether the answer is well-organized with logical flow. Research shows that coherence and relevance are strong signals of problem-solving quality.\n",
    "\n",
    "Relevance (18 points) - Ensures all information pertains to the question, avoiding tangential content that confuses the issue. This maintains focus and efficiency.\n",
    "\n",
    "Conciseness (10 points) - Rewards efficiency by penalizing unnecessary verbosity or repetition while maintaining completeness. This balances against verbose but complete responses.\n",
    "\n",
    "Appropriateness for Context (9 points) â€” Checks whether tone, depth, and format match what the questioner likely needs. Technical questions require different treatment than conversational ones.\"\"\"\n",
    "\n",
    "# Extract model names from candidate_models\n",
    "model_names = [model[\"name\"] for model in candidate_models]\n",
    "result = await query_models(model_names, [user_query]*len(model_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72be76f",
   "metadata": {},
   "source": [
    "# PROMPT BASED SCORING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85fe80c",
   "metadata": {},
   "source": [
    "#### $n^2$ APPROACH\n",
    "Here, if we have $n$ models and model $M_1$ is evaluating model $M_2, M_3,..., M_n$, a single query will be sent for each evaluation i.e. we query $M_1$ with $M_2$'s answer, then query $M_1$ with $M_3$'s answer, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "522a831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING THE QUERIES FOR EACH MODEL TO EVALUATE EACH OTHER\n",
    "\n",
    "\n",
    "scoring_query = lambda answer : f\"\"\"\\\n",
    "You are an expert evaluator for a large language model comparison tool. Your role is to provide an objective, rubric-based score for the candidate's response to a user's query.\n",
    "\n",
    "QUERY:\n",
    "{user_query}\n",
    "\n",
    "CANDIDATE RESPONSE:\n",
    "{answer}\n",
    "\n",
    "RUBRIC:\n",
    "{rubric}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Evaluate the Candidate Response on all rubric dimensions individually, strictly applying the rubric's defined score ranges and weightingsâ€”for example, Correctness & Accuracy is out of 25 points, Completeness 20 points, etc.\n",
    "\n",
    "If the Candidate Response contains any factual inaccuracies, assign the Correctness & Accuracy score corresponding to those errors as explicitly defined in the rubric, which could be as low as 0-4 out of 25 for fundamental factual errors. Do not inflate this score due to other qualities.\n",
    "\n",
    "Calculate the overall score as the weighted sum of all dimension scores, without subjective adjustment or rounding beyond rubric guidance.\n",
    "\n",
    "Your output must be ONLY a JSON object with:\n",
    "\n",
    "1. \"reasoning\": \"<One-sentence justification explicitly referencing rubric criteria and weights, including correctness importance>\",\n",
    "\n",
    "2.\"score\": <integer score from 0 to 100>\n",
    "\n",
    "Use your judgment to apply rubric weightings accurately, and remember that Correctness & Accuracy has the highest impact on the overall score.\n",
    "\"\"\"\n",
    "\n",
    "new_models_to_use = []\n",
    "new_queries_to_use = []\n",
    "models_being_evaluated = []\n",
    "for model1 in model_names:\n",
    "    for item in result:\n",
    "        model2, answer = item[\"model\"], item[\"response\"]\n",
    "        if model1 != model2:\n",
    "            new_models_to_use.append(model1)\n",
    "            new_queries_to_use.append(scoring_query(answer))\n",
    "            models_being_evaluated.append(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d740aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EACH MODEL TO EVALUATE EACH OTHER MODEL'S ANSWER\n",
    "\n",
    "scoring_results = await query_models(new_models_to_use, new_queries_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba748957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "\n",
      "\n",
      "{\n",
      "  \"reasoning\": \"Candidate response demonstrates strong completeness (18/20), clarity (18/18), and relevance (18/18) with appropriate context (9/9), but contains significant factual errors in celestial mechanics (gravity-counteracting forces and mass-velocity relationship), lowering correctness (18/25). Conciseness penalized (7/10) for minor orbit-type over-explanation.\",\n",
      "  \"score\": 88\n",
      "}\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"The response scores highly in Correctness & Accuracy (25/25) with no factual errors, Completeness (20/20) covering all aspects thoroughly, Clarity & Coherence (17/18) due to excellent organization but minor technical density, Relevance (18/18), Conciseness (9/10) with slight verbosity in analogy, and Appropriateness (9/9), totaling 98/100. Correctness & Accuracy is prioritized per rubric weighting.\",\n",
      "  \"score\": 98\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"The candidate response scores highly across all dimensions, particularly in Correctness & Accuracy, which is weighted at 25 points and is crucial for a factual query like this, with the response demonstrating a thorough understanding of planetary orbits without any notable inaccuracies, thus deserving a high score in this category, and when combined with strong showings in Completeness, Clarity & Coherence, Relevance, Conciseness, and Appropriateness for Context, the overall score reflects a well-rounded and accurate response.\",\n",
      "  \"score\": 98\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"The candidate response is evaluated based on the rubric criteria, with Correctness & Accuracy given the highest weight of 25 points due to its critical importance in ensuring the response is factually accurate and free of hallucinations, followed by assessments of Completeness, Clarity & Coherence, Relevance, Conciseness, and Appropriateness for Context, leading to a weighted sum that determines the overall score.\",\n",
      "  \"score\": 98\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "{\"reasoning\":\"The response fully satisfies the rubricâ€™s highestâ€‘weighted Correctness & Accuracy criterion (25/25) with no factual errors, and it also meets Completeness (20/20), Clarity & Coherence (18/18), Relevance (18/18), Conciseness (10/10), and Appropriateness for Context (9/9), yielding a perfect overall score.\", \"score\":100}\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "{\n",
      "  \"reasoning\": \"The answer is largely factually correct and covers the main concepts of orbital mechanics, earning a high Correctness score while a few minor inaccuracies slightly lower it; it meets completeness, clarity, relevance, conciseness, and appropriateness metrics well, leading to an overall score of 83 out of 100.\",\n",
      "  \"score\": 83\n",
      "}\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# RESPONSES & SCORES\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "i = 0\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        if model1 != model2:\n",
    "            print(f\"Judge Model: {model1}\\n\")\n",
    "            print(f\"Evaluated Model: {model2}\\n\")\n",
    "            print(f\"Evaluated Model's Answer:\")\n",
    "            print(scoring_results[i][\"response\"])\n",
    "            i += 1\n",
    "            print(\"--------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5899dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Evaluated Model (Column)</th>\n",
       "      <th>TNG: DeepSeek R1T2 Chimera (free)</th>\n",
       "      <th>Meta: Llama 3.3 70B Instruct (free)</th>\n",
       "      <th>OpenAI: gpt-oss-20b (free)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge Model (Row)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TNG: DeepSeek R1T2 Chimera (free)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta: Llama 3.3 70B Instruct (free)</th>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenAI: gpt-oss-20b (free)</th>\n",
       "      <td>100.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Evaluated Model (Column)             TNG: DeepSeek R1T2 Chimera (free)  \\\n",
       "Judge Model (Row)                                                        \n",
       "TNG: DeepSeek R1T2 Chimera (free)                                  NaN   \n",
       "Meta: Llama 3.3 70B Instruct (free)                               98.0   \n",
       "OpenAI: gpt-oss-20b (free)                                       100.0   \n",
       "\n",
       "Evaluated Model (Column)             Meta: Llama 3.3 70B Instruct (free)  \\\n",
       "Judge Model (Row)                                                          \n",
       "TNG: DeepSeek R1T2 Chimera (free)                                   88.0   \n",
       "Meta: Llama 3.3 70B Instruct (free)                                  NaN   \n",
       "OpenAI: gpt-oss-20b (free)                                          83.0   \n",
       "\n",
       "Evaluated Model (Column)             OpenAI: gpt-oss-20b (free)  \n",
       "Judge Model (Row)                                                \n",
       "TNG: DeepSeek R1T2 Chimera (free)                          98.0  \n",
       "Meta: Llama 3.3 70B Instruct (free)                        98.0  \n",
       "OpenAI: gpt-oss-20b (free)                                  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BUILD RUBRIC FROM MODELS EVALUATIONS OF EACH OTHER\n",
    "\n",
    "scores_table = pd.DataFrame(\n",
    "    np.nan, \n",
    "    index=pd.Index(model_names, name=\"Judge Model (Row)\"),\n",
    "    columns=pd.Index(model_names, name=\"Evaluated Model (Column)\")\n",
    ")\n",
    "i = 0\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        if model1 != model2:\n",
    "            extracted_json = extract_outermost_json(scoring_results[i][\"response\"])\n",
    "            if extracted_json is not None and \"score\" in extracted_json.keys():\n",
    "                scores_table.loc[model1, model2] = round(int(extracted_json[\"score\"]), 2)\n",
    "            i += 1\n",
    "display(scores_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c230ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- WINNING RESPONSE ----------\n",
      "TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Planets orbit the Sun due to **gravity** and **inertia**, following predictable paths governed by the laws of motion and gravity. Here's a step-by-step explanation:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Newtonâ€™s Law of Universal Gravitation**  \n",
      "   - The Sunâ€™s immense mass creates a **gravitational pull** that attracts all planets toward it.  \n",
      "   - This force weakens with distance but is strong enough to keep planets bound to the Sun over vast distances.  \n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Inertia and Orbital Motion**  \n",
      "   - Planets also possess **inertia**â€”the tendency to move in a straight line at constant speed.  \n",
      "   - Gravity bends this straight-line motion into a curved path (orbit). The planet continually \"falls\" toward the Sun but misses it because of its **sideways velocity**.  \n",
      "\n",
      "   ðŸŒ **Example**:  \n",
      "   Imagine swinging a ball on a stringâ€”the string (gravity) pulls the ball inward, while the ballâ€™s speed (inertia) keeps it moving tangentially. A planet orbits the same way, but without a physical \"string.\"\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Orbital Shapes: Keplerâ€™s Laws**  \n",
      "   Johannes Keplerâ€™s laws describe planetary orbits precisely:  \n",
      "   - **First Law**: Orbits are **elliptical** (oval-shaped), with the Sun at one focus.  \n",
      "     - Not perfectly circular (e.g., Mercuryâ€™s orbit is more elliptical than Earthâ€™s).  \n",
      "   - **Second Law**: Planets sweep equal areas in equal time.  \n",
      "     - Planets move **faster** when closer to the Sun (perihelion) and **slower** when farther (aphelion).  \n",
      "   - **Third Law**: The farther a planet is from the Sun, the **longer its orbital period** (year).  \n",
      "     - Formula: \\( T^2 \\propto a^3 \\), where \\(T\\) = orbital period and \\(a\\) = average distance from the Sun.  \n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Stable Orbits: Balancing Gravity and Velocity**  \n",
      "   - **Too slow**: Gravity overpowers inertia, pulling the planet into the Sun.  \n",
      "   - **Too fast**: Inertia wins, flinging the planet out of the Solar System.  \n",
      "   - **Just right**: Gravity and inertia balance, creating a stable, repeating orbit.  \n",
      "\n",
      "    **Orbital Velocity Example**:\n",
      "   - Earth: ~30 km/s  \n",
      "   - Mercury (closer to Sun): ~47 km/s  \n",
      "   - Neptune (farther): ~5 km/s  \n",
      "\n",
      "---\n",
      "\n",
      "### 5. **Why Planets Donâ€™t Collide with the Sun**  \n",
      "   - The sideways (tangential) velocity of planets ensures they perpetually \"miss\" the Sun as gravity curves their path.  \n",
      "   - Orbits formed from the **conservation of angular momentum** during the Solar Systemâ€™s birth.  \n",
      "\n",
      "---\n",
      "\n",
      "### 6. **Orbital Plane**  \n",
      "   - Most planets orbit in the same flat **ecliptic plane**, a remnant of the spinning protoplanetary disk from which the Solar System formed.  \n",
      "   - Exceptions (like Pluto) have more tilted orbits due to past gravitational interactions.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. **Einsteinâ€™s Perspective**  \n",
      "   - General Relativity refines orbits by treating gravity as **spacetime curvature**.  \n",
      "   - Explains subtle effects like Mercuryâ€™s orbital precession, not fully accounted for by Newtonian physics.\n",
      "\n",
      "---\n",
      "\n",
      "**Key Takeaway**:  \n",
      "Planets orbit the Sun because **gravity pulls them inward**, while their **inertia carries them forward**, creating a curved path. This delicate balance maintains stable orbits shaped by the Sunâ€™s mass and each planetâ€™s distance and speed.\n"
     ]
    }
   ],
   "source": [
    "# WINNING MODEL\n",
    "\n",
    "max_mean = -1\n",
    "max_models = []\n",
    "for model in model_names:\n",
    "    mean_score = scores_table[model].mean()\n",
    "    if mean_score > max_mean:\n",
    "        max_mean = mean_score\n",
    "        max_models = [model]\n",
    "    elif mean_score == max_mean:\n",
    "        max_models.append(model)\n",
    "\n",
    "if len(max_models) == 1:\n",
    "    max_model = max_models[0]\n",
    "    print(\"---------- WINNING RESPONSE ----------\")\n",
    "    print(max_model)\n",
    "    result_dict = {item[\"model\"]: item[\"response\"] for item in result}\n",
    "    print(result_dict[max_model])\n",
    "else:\n",
    "    print(\"Tie detected between the following models:\")\n",
    "    print(max_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687e175",
   "metadata": {},
   "source": [
    "#### $n$ APPROACH\n",
    "Here, if we have $n$ models and model $M_1$ is evaluating model $M_2, M_3,..., M_n$, a single query will be sent for all evaluations i.e. we query $M_1$ with $M_2, M_3,..., M_n$'s answers, then query $M_2$ with $M_1, M_3,..., M_n$'s answer, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bc51633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING THE QUERIES FOR EACH MODEL TO EVALUATE EACH OTHER\n",
    "\n",
    "def scoring_query(model):\n",
    "\n",
    "    answers = \"\"\n",
    "    for other_model in result:\n",
    "        if model != other_model[\"model\"]:\n",
    "            answers += f\"{other_model[\"model\"]} RESPONSE:\\n\" + other_model[\"response\"] + \"\\n\\n\"\n",
    "\n",
    "    return f\"\"\"\\\n",
    "You are an expert evaluator for a large language model comparison tool. Your role is to provide objective, rubric-based scores for the candidate's responses to a user's query.\n",
    "\n",
    "QUERY:\n",
    "{user_query}\n",
    "\n",
    "{answers}\n",
    "\n",
    "RUBRIC:\n",
    "{rubric}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Evaluate all the Candidates Responses on all rubric dimensions individually, strictly applying the rubric's defined score ranges and weightingsâ€”for example, Correctness & Accuracy is out of 25 points, Completeness 20 points, etc.\n",
    "\n",
    "If any of the Candidates Responses contain any factual inaccuracies, assign the Correctness & Accuracy score corresponding to those errors as explicitly defined in the rubric, which could be as low as 0-4 out of 25 for fundamental factual errors. Do not inflate this score due to other qualities.\n",
    "\n",
    "Calculate the overall score as the weighted sum of all dimension scores for each Candidate Response, without subjective adjustment or rounding beyond rubric guidance.\n",
    "\n",
    "Your output must be ONLY a JSON object with:\n",
    "\n",
    "1. \"model\": \"<full and exact name of the model as provided in this prompt>\"\n",
    "\n",
    "1.1. \"reasoning\": \"<One-sentence justification explicitly referencing rubric criteria and weights, including correctness importance>\",\n",
    "\n",
    "1.2. \"score\": <integer score from 0 to 100>\n",
    "\n",
    "E.g. {{\"<name of model1>\": {{\"reasoning\": \"<reasoning for model1>\", \"score\": \"<score for model1>\"}}, \"<name of model2>\": {{\"reasoning\": \"<reasoning for model2>\", \"score\": \"<score for model2>\"}}}}\n",
    "\n",
    "Here are the model names for reference: {model_names}\n",
    "\n",
    "Use your judgment to apply rubric weightings accurately, and remember that Correctness & Accuracy has the highest impact on the overall score.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "new_queries_to_use = []\n",
    "for model in model_names:\n",
    "    new_queries_to_use.append(scoring_query(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c91b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EACH MODEL TO EVALUATE EACH OTHER MODEL'S ANSWER\n",
    "\n",
    "scoring_results = await query_models(model_names, new_queries_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e1aa41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "Factual errors in orbital mechanics (mass effect on orbits, elliptical orbit causes) reduce Correctness score. Strong completeness and organization partially compensate.\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "Flawless Correctness & Accuracy (25/25), exceptional Completeness (20/20), and precise mathematical treatment maximize all rubric categories.\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "This response scores high in Correctness & Accuracy (25/25) due to its factual accuracy, Completeness (20/20) as it thoroughly addresses the query, Clarity & Coherence (18/18) for its logical flow, Relevance (18/18) by staying on topic, and Conciseness (8/10) for being detailed but slightly verbose, with Appropriateness for Context (9/9) due to its technical depth matching the question's needs. The overall score is calculated as (25*0.25 + 20*0.20 + 18*0.18 + 18*0.18 + 8*0.10 + 9*0.09) = 6.25 + 4 + 3.24 + 3.24 + 0.8 + 0.81 = 18.34, then normalized to 100 scale as 92.\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "This response also scores high in Correctness & Accuracy (25/25) for its accuracy, Completeness (19/20) as it covers the query well but with minor omissions, Clarity & Coherence (17/18) for its good flow, Relevance (17/18) by staying mostly on topic, and Conciseness (7/10) for being somewhat verbose, with Appropriateness for Context (8/9) due to its generally matching the question's needs but with slight deviations. The overall score is calculated as (25*0.25 + 19*0.20 + 17*0.18 + 17*0.18 + 7*0.10 + 8*0.09) = 6.25 + 3.8 + 3.06 + 3.06 + 0.7 + 0.72 = 17.59, then normalized to 100 scale as 88.\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "The DeepSeek response demonstrates flawless correctness (25/25), full completeness (20/20), superior clarity and relevance (18/18 each), and strong conciseness (8/10) with only minor verbosity, yielding a high overall score of 98/100.\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "The Llama answer contains factual inaccuracies regarding planet mass and orbital shape, reducing its correctness to 15/25, but remains fully complete, moderately coherent, entirely relevant, and moderately concise, giving an overall score of 85/100.\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# RESPONSES & SCORES\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "for i in range(len(model_names)):\n",
    "    extracted_json = extract_outermost_json(scoring_results[i][\"response\"])\n",
    "    if extracted_json is not None:\n",
    "        for evaluated_model in extracted_json.keys():\n",
    "            print(f\"Judge Model: {model_names[i]}\\n\")\n",
    "            print(f\"Evaluated Model: {evaluated_model}\\n\")\n",
    "            print(f\"Evaluated Model's Answer:\")\n",
    "            print(extracted_json[evaluated_model][\"reasoning\"])\n",
    "            print(\"--------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d664300f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Evaluated Model (Column)</th>\n",
       "      <th>TNG: DeepSeek R1T2 Chimera (free)</th>\n",
       "      <th>Meta: Llama 3.3 70B Instruct (free)</th>\n",
       "      <th>OpenAI: gpt-oss-20b (free)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge Model (Row)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TNG: DeepSeek R1T2 Chimera (free)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>79.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta: Llama 3.3 70B Instruct (free)</th>\n",
       "      <td>92.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenAI: gpt-oss-20b (free)</th>\n",
       "      <td>98.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Evaluated Model (Column)             TNG: DeepSeek R1T2 Chimera (free)  \\\n",
       "Judge Model (Row)                                                        \n",
       "TNG: DeepSeek R1T2 Chimera (free)                                  NaN   \n",
       "Meta: Llama 3.3 70B Instruct (free)                               92.0   \n",
       "OpenAI: gpt-oss-20b (free)                                        98.0   \n",
       "\n",
       "Evaluated Model (Column)             Meta: Llama 3.3 70B Instruct (free)  \\\n",
       "Judge Model (Row)                                                          \n",
       "TNG: DeepSeek R1T2 Chimera (free)                                   79.0   \n",
       "Meta: Llama 3.3 70B Instruct (free)                                  NaN   \n",
       "OpenAI: gpt-oss-20b (free)                                          85.0   \n",
       "\n",
       "Evaluated Model (Column)             OpenAI: gpt-oss-20b (free)  \n",
       "Judge Model (Row)                                                \n",
       "TNG: DeepSeek R1T2 Chimera (free)                         100.0  \n",
       "Meta: Llama 3.3 70B Instruct (free)                        88.0  \n",
       "OpenAI: gpt-oss-20b (free)                                  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BUILD RUBRIC FROM MODELS EVALUATIONS OF EACH OTHER\n",
    "\n",
    "scores_table = pd.DataFrame(\n",
    "    np.nan, \n",
    "    index=pd.Index(model_names, name=\"Judge Model (Row)\"),\n",
    "    columns=pd.Index(model_names, name=\"Evaluated Model (Column)\")\n",
    ")\n",
    "i = 0\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        if model1 != model2:\n",
    "            extracted_json = extract_outermost_json(scoring_results[i][\"response\"])[model2]\n",
    "            if extracted_json is not None and \"score\" in extracted_json.keys():\n",
    "                scores_table.loc[model1, model2] = round(extracted_json[\"score\"], 2)\n",
    "    i += 1\n",
    "display(scores_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd215a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- WINNING RESPONSE ----------\n",
      "TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Planets orbit the Sun due to **gravity** and **inertia**, following predictable paths governed by the laws of motion and gravity. Here's a step-by-step explanation:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Newtonâ€™s Law of Universal Gravitation**  \n",
      "   - The Sunâ€™s immense mass creates a **gravitational pull** that attracts all planets toward it.  \n",
      "   - This force weakens with distance but is strong enough to keep planets bound to the Sun over vast distances.  \n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Inertia and Orbital Motion**  \n",
      "   - Planets also possess **inertia**â€”the tendency to move in a straight line at constant speed.  \n",
      "   - Gravity bends this straight-line motion into a curved path (orbit). The planet continually \"falls\" toward the Sun but misses it because of its **sideways velocity**.  \n",
      "\n",
      "   ðŸŒ **Example**:  \n",
      "   Imagine swinging a ball on a stringâ€”the string (gravity) pulls the ball inward, while the ballâ€™s speed (inertia) keeps it moving tangentially. A planet orbits the same way, but without a physical \"string.\"\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Orbital Shapes: Keplerâ€™s Laws**  \n",
      "   Johannes Keplerâ€™s laws describe planetary orbits precisely:  \n",
      "   - **First Law**: Orbits are **elliptical** (oval-shaped), with the Sun at one focus.  \n",
      "     - Not perfectly circular (e.g., Mercuryâ€™s orbit is more elliptical than Earthâ€™s).  \n",
      "   - **Second Law**: Planets sweep equal areas in equal time.  \n",
      "     - Planets move **faster** when closer to the Sun (perihelion) and **slower** when farther (aphelion).  \n",
      "   - **Third Law**: The farther a planet is from the Sun, the **longer its orbital period** (year).  \n",
      "     - Formula: \\( T^2 \\propto a^3 \\), where \\(T\\) = orbital period and \\(a\\) = average distance from the Sun.  \n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Stable Orbits: Balancing Gravity and Velocity**  \n",
      "   - **Too slow**: Gravity overpowers inertia, pulling the planet into the Sun.  \n",
      "   - **Too fast**: Inertia wins, flinging the planet out of the Solar System.  \n",
      "   - **Just right**: Gravity and inertia balance, creating a stable, repeating orbit.  \n",
      "\n",
      "    **Orbital Velocity Example**:\n",
      "   - Earth: ~30 km/s  \n",
      "   - Mercury (closer to Sun): ~47 km/s  \n",
      "   - Neptune (farther): ~5 km/s  \n",
      "\n",
      "---\n",
      "\n",
      "### 5. **Why Planets Donâ€™t Collide with the Sun**  \n",
      "   - The sideways (tangential) velocity of planets ensures they perpetually \"miss\" the Sun as gravity curves their path.  \n",
      "   - Orbits formed from the **conservation of angular momentum** during the Solar Systemâ€™s birth.  \n",
      "\n",
      "---\n",
      "\n",
      "### 6. **Orbital Plane**  \n",
      "   - Most planets orbit in the same flat **ecliptic plane**, a remnant of the spinning protoplanetary disk from which the Solar System formed.  \n",
      "   - Exceptions (like Pluto) have more tilted orbits due to past gravitational interactions.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. **Einsteinâ€™s Perspective**  \n",
      "   - General Relativity refines orbits by treating gravity as **spacetime curvature**.  \n",
      "   - Explains subtle effects like Mercuryâ€™s orbital precession, not fully accounted for by Newtonian physics.\n",
      "\n",
      "---\n",
      "\n",
      "**Key Takeaway**:  \n",
      "Planets orbit the Sun because **gravity pulls them inward**, while their **inertia carries them forward**, creating a curved path. This delicate balance maintains stable orbits shaped by the Sunâ€™s mass and each planetâ€™s distance and speed.\n"
     ]
    }
   ],
   "source": [
    "# WINNING MODEL\n",
    "\n",
    "max_mean = -1\n",
    "max_models = []\n",
    "for model in model_names:\n",
    "    mean_score = scores_table[model].mean()\n",
    "    if mean_score > max_mean:\n",
    "        max_mean = mean_score\n",
    "        max_models = [model]\n",
    "    elif mean_score == max_mean:\n",
    "        max_models.append(model)\n",
    "\n",
    "if len(max_models) == 1:\n",
    "    max_model = max_models[0]\n",
    "    print(\"---------- WINNING RESPONSE ----------\")\n",
    "    print(max_model)\n",
    "    result_dict = {item[\"model\"]: item[\"response\"] for item in result}\n",
    "    print(result_dict[max_model])\n",
    "else:\n",
    "    print(\"Tie detected between the following models:\")\n",
    "    print(max_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbb0ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
