{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23bfbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import asyncio\n",
    "import re\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74acda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in .env file or environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9189a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=OPENROUTER_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b00259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL AVAILABLE MODELS FOR TESTING\n",
    "# *to add models, use the screen name for the model from OpenRouter as the key and\n",
    "# the actual name used in the api as the value\n",
    "# *comment out models that you don't want to be used\n",
    "\n",
    "candidate_models = [\n",
    "    # PAID\n",
    "    # {\"name\": \"Anthropic: Claude Sonnet 4.5\", \"openrouter\": \"anthropic/claude-sonnet-4.5\"},\n",
    "    # {\"name\": \"OpenAI: GPT-4o\", \"openrouter\": \"openai/gpt-4o\"},\n",
    "    # {\"name\": \"OpenAI: GPT-4o Mini\", \"openrouter\": \"openai/gpt-4o-mini\"}\n",
    "    # FREE\n",
    "    # {\"name\": \"MiniMax: MiniMax M2 (free)\", \"openrouter\": \"minimax/minimax-m2:free\"}, # Not working for me (OWEN)\n",
    "    {\"name\": \"TNG: DeepSeek R1T2 Chimera (free)\", \"openrouter\": \"tngtech/deepseek-r1t2-chimera:free\"},\n",
    "    {\"name\": \"Meta: Llama 3.3 70B Instruct (free)\", \"openrouter\": \"meta-llama/llama-3.3-70b-instruct:free\"},\n",
    "    {\"name\": \"OpenAI: gpt-oss-20b (free)\", \"openrouter\": \"openai/gpt-oss-20b:free\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72eeda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY FUNCTIONS\n",
    "\n",
    "async def query_model(model_name: str, query: str, role=\"user\"):\n",
    "    \"\"\"\n",
    "    Queries a single model using the models in 'candidate_models'\n",
    "    Args:\n",
    "        model_name: The name of the model (from candidate_models \"name\" field)\n",
    "    Returns: dict with keys 'model' and 'response'\n",
    "    \"\"\"\n",
    "    model_dict = next((m for m in candidate_models if m[\"name\"] == model_name), None)\n",
    "    if model_dict is None:\n",
    "        return {\"model\": model_name, \"response\": f\"Error: Model '{model_name}' not found in candidate_models\"}\n",
    "    \n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=model_dict[\"openrouter\"],\n",
    "            messages=[{\"role\" : role, \"content\" : query}],\n",
    "            temperature=1\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        return {\"model\": model_name, \"response\": content}\n",
    "    except Exception as e:\n",
    "        return {\"model\": model_name, \"response\": str(e)}\n",
    "\n",
    "async def query_models(model_names: list[str], queries: list[str], role=\"user\"):\n",
    "    \"\"\"\n",
    "    Queries multiple models asynchronously\n",
    "    Args:\n",
    "        model_names: List of model names (from candidate_models \"name\" field)\n",
    "    \"\"\"\n",
    "    coroutines = [query_model(model_names[i], queries[i], role=role) for i in range(len(model_names))]\n",
    "    results = await asyncio.gather(*coroutines)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON PARSER\n",
    "\n",
    "def extract_outermost_json(text):\n",
    "    \"\"\"\n",
    "    Extracts the outermost JSON object from arbitrary text.\n",
    "    Returns the parsed JSON (dict/list) or raises ValueError if no valid JSON found.\n",
    "    \"\"\"\n",
    "\n",
    "    start = None\n",
    "    depth = 0\n",
    "    in_string = False\n",
    "    escape = False\n",
    "\n",
    "    for i, ch in enumerate(text):\n",
    "        if escape:\n",
    "            escape = False\n",
    "            continue\n",
    "\n",
    "        if ch == \"\\\\\":\n",
    "            escape = True\n",
    "            continue\n",
    "\n",
    "        if ch == '\"' and not escape:\n",
    "            in_string = not in_string\n",
    "            continue\n",
    "\n",
    "        if not in_string:\n",
    "            if ch == \"{\":\n",
    "                if depth == 0:\n",
    "                    start = i\n",
    "                depth += 1\n",
    "            elif ch == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0 and start is not None:\n",
    "                    candidate = text[start:i+1]\n",
    "                    try:\n",
    "                        return json.loads(candidate)\n",
    "                    except Exception:\n",
    "                        # continue scanning if not valid\n",
    "                        pass\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2757c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EACH MODEL FOR THEIR ANSWER TO THE USER PROMPT\n",
    "\n",
    "user_query = \"Explain how planets orbit around the sun\"\n",
    "\n",
    "rubric = \"\"\"Correctness & Accuracy (25 points) — Ensures claims are factually accurate and verifiable, addressing the most critical concern of hallucination-free responses. This is weighted highest because inaccurate information undermines all other qualities.\n",
    "\n",
    "Completeness (20 points) - Verifies the answer addresses all aspects of the query without significant omissions. This prevents shallow or partial responses that technically answer only part of the question.\n",
    "\n",
    "Clarity & Coherence (18 points) - Assesses whether the answer is well-organized with logical flow. Research shows that coherence and relevance are strong signals of problem-solving quality.\n",
    "\n",
    "Relevance (18 points) - Ensures all information pertains to the question, avoiding tangential content that confuses the issue. This maintains focus and efficiency.\n",
    "\n",
    "Conciseness (10 points) - Rewards efficiency by penalizing unnecessary verbosity or repetition while maintaining completeness. This balances against verbose but complete responses.\n",
    "\n",
    "Appropriateness for Context (9 points) — Checks whether tone, depth, and format match what the questioner likely needs. Technical questions require different treatment than conversational ones.\"\"\"\n",
    "\n",
    "# Extract model names from candidate_models\n",
    "model_names = [model[\"name\"] for model in candidate_models]\n",
    "result = await query_models(model_names, [user_query]*len(model_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72be76f",
   "metadata": {},
   "source": [
    "# PROMPT BASED SCORING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85fe80c",
   "metadata": {},
   "source": [
    "#### $n^2$ APPROACH\n",
    "Here, if we have $n$ models and model $M_1$ is evaluating model $M_2, M_3,..., M_n$, a single query will be sent for each evaluation i.e. we query $M_1$ with $M_2$'s answer, then query $M_1$ with $M_3$'s answer, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522a831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING THE QUERIES FOR EACH MODEL TO EVALUATE EACH OTHER\n",
    "\n",
    "\n",
    "scoring_query = lambda answer : f\"\"\"\\\n",
    "You are an expert evaluator for a large language model comparison tool. Your role is to provide an objective, rubric-based score for the candidate's response to a user's query.\n",
    "\n",
    "QUERY:\n",
    "{user_query}\n",
    "\n",
    "CANDIDATE RESPONSE:\n",
    "{answer}\n",
    "\n",
    "RUBRIC:\n",
    "{rubric}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Evaluate the Candidate Response on all rubric dimensions individually, strictly applying the rubric's defined score ranges and weightings—for example, Correctness & Accuracy is out of 25 points, Completeness 20 points, etc.\n",
    "\n",
    "If the Candidate Response contains any factual inaccuracies, assign the Correctness & Accuracy score corresponding to those errors as explicitly defined in the rubric, which could be as low as 0-4 out of 25 for fundamental factual errors. Do not inflate this score due to other qualities.\n",
    "\n",
    "Calculate the overall score as the weighted sum of all dimension scores, without subjective adjustment or rounding beyond rubric guidance.\n",
    "\n",
    "Your output must be ONLY a JSON object with:\n",
    "\n",
    "1. \"reasoning\": \"<One-sentence justification explicitly referencing rubric criteria and weights, including correctness importance>\",\n",
    "\n",
    "2.\"score\": <integer score from 0 to 100>\n",
    "\n",
    "Use your judgment to apply rubric weightings accurately, and remember that Correctness & Accuracy has the highest impact on the overall score.\n",
    "\"\"\"\n",
    "\n",
    "new_models_to_use = []\n",
    "new_queries_to_use = []\n",
    "models_being_evaluated = []\n",
    "for model1 in model_names:\n",
    "    for item in result:\n",
    "        model2, answer = item[\"model\"], item[\"response\"]\n",
    "        if model1 != model2:\n",
    "            new_models_to_use.append(model1)\n",
    "            new_queries_to_use.append(scoring_query(answer))\n",
    "            models_being_evaluated.append(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d740aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EACH MODEL TO EVALUATE EACH OTHER MODEL'S ANSWER\n",
    "\n",
    "scoring_results = await query_models(new_models_to_use, new_queries_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba748957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "\n",
      "{\n",
      "  \"reasoning\": \"Correctness & Accuracy: Minor factual error (centrifugal force vs. centripetal force), so moderate deduction (20/25). All other dimensions meet rubric standards for full points. Correctness has highest impact.\",\n",
      "  \"score\": 95\n",
      "}\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"The response scores high in Correctness & Accuracy (25/25) with verified explanations of orbital mechanics, full marks in Completeness (20/20) for covering all key aspects, strong Clarity & Coherence (18/18) through logical structure, full Relevance (18/18) with no tangents, slightly reduced Conciseness (9/10) due to moderate verbosity, and full Appropriateness (9/9) for technical formatting. Correctness's high weighting (25%) ensures no penalty as no inaccuracies were present.\",\n",
      "  \"score\": 95\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"The response is evaluated based on the rubric, with Correctness & Accuracy given the highest weight of 25 points due to its crucial importance in providing hallucination-free and factually accurate information, followed by assessments of Completeness, Clarity & Coherence, Relevance, Conciseness, and Appropriateness for Context, with the overall score calculated as a weighted sum of these dimensions\",\n",
      "  \"score\": 98\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"The Candidate Response is evaluated based on the rubric, with high scores in Correctness & Accuracy, Completeness, Clarity & Coherence, Relevance, and Appropriateness for Context, but slightly reduced for Conciseness due to extensive detail, resulting in an overall score calculated as (23/25 * 0.25) + (19/20 * 0.20) + (17/18 * 0.18) + (17/18 * 0.18) + (8/10 * 0.10) + (8/9 * 0.09) = 0.25*23 + 0.20*19 + 0.18*17 + 0.18*17 + 0.10*8 + 0.09*8 = 5.75 + 3.8 + 3.06 + 3.06 + 0.8 + 0.72 = 17.19, then scaled to 0-100 range, considering the rubric's emphasis on Correctness & Accuracy and the overall performance across dimensions\",\n",
      "  \"score\": 96\n",
      "}\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model: TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "{\"reasoning\":\"The response provides entirely correct, comprehensive, clear, relevant, concise, and appropriately detailed information; each dimension scores full marks, especially correctness (critical 25/25).\",\"score\":100}\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Judge Model: OpenAI: gpt-oss-20b (free)\n",
      "\n",
      "Evaluated Model: Meta: Llama 3.3 70B Instruct (free)\n",
      "\n",
      "Evaluated Model's Answer:\n",
      "{\"reasoning\":\"Correctness & Accuracy (25/25) – all factual claims are accurate about gravity, elliptical orbits, Kepler’s laws; Completeness (20/20) – covers principle, orbit shape, key components, physical balance, and laws; Clarity & Coherence (18/18) – well‑structured sections with logical flow; Relevance (18/18) – no irrelevant tangents; Conciseness (10/10) – efficient though slightly verbose, still within acceptable length; Appropriateness (9/9) – tone and depth suitable for a general explanatory query. Highest weighted correctness and full marks in all dimensions produce a total score of 100.\"}\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# RESPONSES & SCORES\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "i = 0\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        if model1 != model2:\n",
    "            print(f\"Judge Model: {model1}\\n\")\n",
    "            print(f\"Evaluated Model: {model2}\\n\")\n",
    "            print(f\"Evaluated Model's Answer:\")\n",
    "            print(scoring_results[i][\"response\"])\n",
    "            i += 1\n",
    "            print(\"--------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5899dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Evaluated Model (Column)</th>\n",
       "      <th>TNG: DeepSeek R1T2 Chimera (free)</th>\n",
       "      <th>Meta: Llama 3.3 70B Instruct (free)</th>\n",
       "      <th>OpenAI: gpt-oss-20b (free)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge Model (Row)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TNG: DeepSeek R1T2 Chimera (free)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta: Llama 3.3 70B Instruct (free)</th>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenAI: gpt-oss-20b (free)</th>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Evaluated Model (Column)             TNG: DeepSeek R1T2 Chimera (free)  \\\n",
       "Judge Model (Row)                                                        \n",
       "TNG: DeepSeek R1T2 Chimera (free)                                  NaN   \n",
       "Meta: Llama 3.3 70B Instruct (free)                               98.0   \n",
       "OpenAI: gpt-oss-20b (free)                                       100.0   \n",
       "\n",
       "Evaluated Model (Column)             Meta: Llama 3.3 70B Instruct (free)  \\\n",
       "Judge Model (Row)                                                          \n",
       "TNG: DeepSeek R1T2 Chimera (free)                                   95.0   \n",
       "Meta: Llama 3.3 70B Instruct (free)                                  NaN   \n",
       "OpenAI: gpt-oss-20b (free)                                           NaN   \n",
       "\n",
       "Evaluated Model (Column)             OpenAI: gpt-oss-20b (free)  \n",
       "Judge Model (Row)                                                \n",
       "TNG: DeepSeek R1T2 Chimera (free)                          95.0  \n",
       "Meta: Llama 3.3 70B Instruct (free)                        96.0  \n",
       "OpenAI: gpt-oss-20b (free)                                  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BUILD RUBRIC FROM MODELS EVALUATIONS OF EACH OTHER\n",
    "\n",
    "scores_table = pd.DataFrame(\n",
    "    np.nan, \n",
    "    index=pd.Index(model_names, name=\"Judge Model (Row)\"),\n",
    "    columns=pd.Index(model_names, name=\"Evaluated Model (Column)\")\n",
    ")\n",
    "i = 0\n",
    "for model1 in model_names:\n",
    "    for model2 in model_names:\n",
    "        if model1 != model2:\n",
    "            extracted_json = extract_outermost_json(scoring_results[i][\"response\"])\n",
    "            if \"score\" in extracted_json.keys():\n",
    "                score_value = extract_outermost_json(scoring_results[i][\"response\"])[\"score\"]\n",
    "            else:\n",
    "                score_value = np.nan\n",
    "            if not np.isnan(score_value):\n",
    "                scores_table.loc[model1, model2] = round(int(score_value), 2)\n",
    "            else:\n",
    "                scores_table.loc[model1, model2] = np.nan\n",
    "            i += 1\n",
    "display(scores_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c230ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- WINNING RESPONSE ----------\n",
      "TNG: DeepSeek R1T2 Chimera (free)\n",
      "\n",
      "\n",
      "Planets orbit the Sun due to a balance between **gravity** and **inertia**, following precise laws of physics. Here's a step-by-step explanation:\n",
      "\n",
      "### 1. **Newton's Law of Universal Gravitation**\n",
      "   - The Sun exerts a **gravitational force** on each planet, pulling it toward the Sun's center. This force depends on:\n",
      "     - The masses of the Sun (\\(M\\)) and planet (\\(m\\)).\n",
      "     - The square of the distance (\\(r\\)) between them:  \n",
      "       \\[\n",
      "       F = G \\frac{M \\cdot m}{r^2}\n",
      "       \\]  \n",
      "       (where \\(G\\) is the gravitational constant).\n",
      "\n",
      "### 2. **Inertia and Tangential Velocity**\n",
      "   - Planets have **inertia** (from Newton's first law), meaning they move in a straight line unless acted upon by a force.\n",
      "   - Each planet also possesses **tangential velocity** (perpendicular to the Sun-planet line). Without gravity, this velocity would carry the planet into deep space.\n",
      "   - **Gravity acts as a centripetal force**, continuously deflecting the planet's path into a curved orbit instead of a straight line.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Orbital Shape: Ellipses (Kepler's First Law)**\n",
      "   - Planets follow **elliptical orbits**, with the Sun at one **focus** of the ellipse (not the center).\n",
      "   - **Closest approach**: *Perihelion* (fastest orbital speed).  \n",
      "   **Farthest distance**: *Aphelion* (slowest speed).\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Orbital Speed (Kepler's Second Law)**\n",
      "   - A line connecting the Sun and a planet sweeps **equal areas in equal time intervals**.  \n",
      "     ![Kepler's Second Law](https://www.astronomy.ohio-state.edu/pogge.1/Ast161/Unit4/Images/kepler2.gif)\n",
      "   - **Implication**: Planets move **faster** when closer to the Sun and **slower** when farther away.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. **Orbital Period (Kepler's Third Law)**\n",
      "   - The square of a planet’s orbital period (\\(T\\)) is proportional to the cube of its orbit’s semi-major axis (\\(a\\), half the longest diameter of the ellipse):  \n",
      "     \\[\n",
      "     T^2 \\propto a^3\n",
      "     \\]\n",
      "   - **Example**: Earth (\\(a = 1\\) AU) orbits in 1 year. Mars (\\(a \\approx 1.5\\) AU) takes ~1.88 years.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. **Orbital Stability**\n",
      "   - **Two-body dominance**: While the Sun-planet interaction dominates, minor gravitational tugs from other planets cause slight orbital perturbations (e.g., Mercury’s orbit shifts slightly over time).\n",
      "   - **Barycenter**: The Sun and planets orbit a shared center of mass. For most planets, this point lies near the Sun’s center due to its immense mass.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. **Common Orbital Plane**\n",
      "   - Most planets orbit near the **ecliptic plane** (Earth’s orbital plane), a relic of the solar system's formation from a rotating disk of gas and dust.\n",
      "   - Exceptions: Dwarf planets like Pluto have highly inclined orbits.\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Takeaway**\n",
      "Planetary orbits result from the interplay between **gravity** (pulling toward the Sun) and **inertia/tangential velocity** (pushing \"forward\"). The balance creates stable, elliptical paths described by **Kepler's laws** and explained by **Newtonian physics**. This framework also applies to moons, asteroids, and human-made satellites.\n"
     ]
    }
   ],
   "source": [
    "# WINNING MODEL\n",
    "\n",
    "max_mean = -1\n",
    "max_models = []\n",
    "for model in model_names:\n",
    "    mean_score = scores_table[model].mean()\n",
    "    if mean_score > max_mean:\n",
    "        max_mean = mean_score\n",
    "        max_models = [model]\n",
    "    elif mean_score == max_mean:\n",
    "        max_models.append(model)\n",
    "\n",
    "if len(max_models) == 1:\n",
    "    max_model = max_models[0]\n",
    "    print(\"---------- WINNING RESPONSE ----------\")\n",
    "    print(max_model)\n",
    "    result_dict = {item[\"model\"]: item[\"response\"] for item in result}\n",
    "    print(result_dict[max_model])\n",
    "else:\n",
    "    print(\"Tie detected between the following models:\")\n",
    "    print(max_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687e175",
   "metadata": {},
   "source": [
    "#### $n$ APPROACH\n",
    "Here, if we have $n$ models and model $M_1$ is evaluating model $M_2, M_3,..., M_n$, a single query will be sent for all evaluations i.e. we query $M_1$ with $M_2, M_3,..., M_n$'s answers, then query $M_1, M_3,..., M_n$'s answer, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bc51633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING THE QUERIES FOR EACH MODEL TO EVALUATE EACH OTHER\n",
    "\n",
    "def scoring_query(model):\n",
    "\n",
    "    answers = \"\"\n",
    "    for other_model in result:\n",
    "        if model != other_model[\"model\"]:\n",
    "            answers += f\"{other_model[\"model\"]} RESPONSE:\\n\" + other_model[\"response\"] + \"\\n\\n\"\n",
    "\n",
    "    return f\"\"\"\\\n",
    "You are an expert evaluator for a large language model comparison tool. Your role is to provide objective, rubric-based scores for the candidate's responses to a user's query.\n",
    "\n",
    "QUERY:\n",
    "{user_query}\n",
    "\n",
    "{answers}\n",
    "\n",
    "RUBRIC:\n",
    "{rubric}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Evaluate all the Candidates Responses on all rubric dimensions individually, strictly applying the rubric's defined score ranges and weightings—for example, Correctness & Accuracy is out of 25 points, Completeness 20 points, etc.\n",
    "\n",
    "If any of the Candidates Responses contain any factual inaccuracies, assign the Correctness & Accuracy score corresponding to those errors as explicitly defined in the rubric, which could be as low as 0-4 out of 25 for fundamental factual errors. Do not inflate this score due to other qualities.\n",
    "\n",
    "Calculate the overall score as the weighted sum of all dimension scores for each Candidate Response, without subjective adjustment or rounding beyond rubric guidance.\n",
    "\n",
    "Your output must be ONLY a JSON object with:\n",
    "\n",
    "1. \"model\": \"<name of the model as provided in this prompt>\"\n",
    "\n",
    "1.1. \"reasoning\": \"<One-sentence justification explicitly referencing rubric criteria and weights, including correctness importance>\",\n",
    "\n",
    "1.2. \"score\": <integer score from 0 to 100>\n",
    "\n",
    "E.g. {{\"<name of model1>\": {{\"reasoning\": \"<reasoning for model1>\", \"score\": \"<score for model1>\"}}, \"<name of model2>\": {{\"reasoning\": \"<reasoning for model2>\", \"score\": \"<score for model2>\"}}}}\n",
    "\n",
    "Use your judgment to apply rubric weightings accurately, and remember that Correctness & Accuracy has the highest impact on the overall score.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "new_queries_to_use = []\n",
    "for model in model_names:\n",
    "    new_queries_to_use.append(scoring_query(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c91b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EACH MODEL TO EVALUATE EACH OTHER MODEL'S ANSWER\n",
    "\n",
    "scoring_results = await query_models(model_names, new_queries_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2716a0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Meta: Llama 3.3 70B Instruct (free)\": {\n",
      "        \"reasoning\": \"Highly accurate with minor terminology imprecision (centrifugal force mention), complete coverage of orbital principles, clear organization, and appropriate depth for general audience. Scores highly despite slight accuracy deduction due to heavy 25-point rubric weighting.\",\n",
      "        \"score\": 91\n",
      "    },\n",
      "    \"OpenAI: gpt-oss-20b (free)\": {\n",
      "        \"reasoning\": \"Flawless accuracy with precise physics terminology and equations, exceptionally complete coverage including conservation laws and orbital mechanics mathematics, logically structured with tables/analogies while maintaining relevance. Maximizes weighted rubric impact through perfect correctness score.\",\n",
      "        \"score\": 98\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(extract_outermost_json(scoring_results[0][\"response\"]), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664300f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
