{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f817ce6f",
      "metadata": {},
      "source": [
        "# Multi-Agent Self-Reflection\n",
        "\n",
        "**Key idea**: Employ three LLM \"agents\"—a **Grader**, a **Reflector** (identifies mistakes), and a **Refiner** (updates guidelines)—in a loop to self-improve the grading process.\n",
        "\n",
        "- **Process**:\n",
        "  1. **Grader** assigns grades per rubric.\n",
        "  2. **Reflector** analyzes errors (e.g., mis-scored cases).\n",
        "  3. **Refiner** tweaks the original rubric or prompt to correct these errors.\n",
        "\n",
        "- **Outcome**: Higher alignment with human judgments and reduced bias, even without extra human input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a18a6f9b",
      "metadata": {},
      "source": [
        "## Agent Roles\n",
        "\n",
        "- **Grader Agent**\n",
        "  - Takes: rubric, example, model response.\n",
        "  - Outputs: per-dimension scores + short justification.\n",
        "\n",
        "- **Reflector Agent**\n",
        "  - Takes: original rubric, example, response, Grader scores, human labels (when available).\n",
        "  - Outputs: critique of Grader’s decision, identifies mis-scored cases and patterns.\n",
        "\n",
        "- **Refiner Agent**\n",
        "  - Takes: rubric + Reflector feedback.\n",
        "  - Outputs: updated rubric / grading prompt with improved instructions.\n",
        "\n",
        "## Multi-Agent Loop\n",
        "\n",
        "- **Iteration structure**:\n",
        "  1. Run Grader over a batch of examples using current rubric.\n",
        "  2. Run Reflector to compare Grader vs. human labels and highlight errors.\n",
        "  3. Run Refiner to update the rubric / prompt.\n",
        "  4. Repeat for N rounds or until metrics converge.\n",
        "\n",
        "## Evaluation & Logging\n",
        "\n",
        "- Track agreement with human labels over iterations (accuracy, correlation, etc.).\n",
        "- Log rubric versions and example-level traces for debugging.\n",
        "- Save summaries of how the rubric evolves over time.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
