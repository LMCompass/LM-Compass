{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f18d673",
   "metadata": {},
   "source": [
    "# Rationale-Based Self-Critique Loops (RL4F)\n",
    "\n",
    "**Key idea:** After initial grading, generate a *critique* of the grade in natural language, then allow the model to revise its score in a second pass.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "1. **Grade Prompt**  \n",
    "   > “Here is your score and rationale.”\n",
    "\n",
    "2. **Critique Prompt**  \n",
    "   > “Critique your previous rationale: were you too harsh or lenient?”\n",
    "\n",
    "3. **Revision**  \n",
    "   > “Based on your critique, update the score and rationale.”\n",
    "\n",
    "**Effect:** Mimics human feedback loops; shown to improve output quality across generation tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5deb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_based_evaluator import PromptBasedEvaluator\n",
    "\n",
    "class RL4FEvaluator(PromptBasedEvaluator):\n",
    "    def __init__(self,*model_names):\n",
    "        super().__init__(*model_names)\n",
    "    \n",
    "    def _self_critique_prompt(self, user_query, rubric, response, reasoning, score):\n",
    "        # TODO: Implement the prompt template for self-critique\n",
    "        pass\n",
    "\n",
    "    async def _critique_rationale(self):\n",
    "        # TODO: Ask \"evaluating model\" to critique its \"reasoning\" and \"score\"\n",
    "        #       to the \"evaluated model's\" user query \"response\"\n",
    "        pass\n",
    "    \n",
    "    async def evaluate(self, user_query, rubric, iterations=2):\n",
    "        # Step 1: Initial Evaluation\n",
    "        await self.n_sq_evaluate(user_query, rubric)\n",
    "        # await self.n_evaluate(user_query, rubric)\n",
    "\n",
    "        # Step 2: Critique Rationale\n",
    "        for _ in range(iterations):\n",
    "            await self._critique_rationale()\n",
    "            # Step 3: Re-evaluation\n",
    "            await self.n_sq_evaluate(user_query, rubric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab4a2d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got user query answers.\n",
      "Got scoring results.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "user_query = \"How many gigabytes of VRAM should I have for 1080p gaming?\"\n",
    "\n",
    "rubric = \"\"\"Correctness & Accuracy (25 points) — Ensures claims are factually accurate and verifiable, addressing the most critical concern of hallucination-free responses. This is weighted highest because inaccurate information undermines all other qualities.\n",
    "\n",
    "Completeness (20 points) - Verifies the answer addresses all aspects of the query without significant omissions. This prevents shallow or partial responses that technically answer only part of the question.\n",
    "\n",
    "Clarity & Coherence (18 points) - Assesses whether the answer is well-organized with logical flow. Research shows that coherence and relevance are strong signals of problem-solving quality.\n",
    "\n",
    "Relevance (18 points) - Ensures all information pertains to the question, avoiding tangential content that confuses the issue. This maintains focus and efficiency.\n",
    "\n",
    "Conciseness (10 points) - Rewards efficiency by penalizing unnecessary verbosity or repetition while maintaining completeness. This balances against verbose but complete responses.\n",
    "\n",
    "Appropriateness for Context (9 points) — Checks whether tone, depth, and format match what the questioner likely needs. Technical questions require different treatment than conversational ones.\"\"\"\n",
    "\n",
    "eval = RL4FEvaluator(\n",
    "    \"TNG: DeepSeek R1T2 Chimera (free)\",\n",
    "    \"StepFun: Step 3.5 Flash (free)\",\n",
    "    \"AllenAI: Molmo2 8B (free)\"\n",
    ")\n",
    "\n",
    "await eval.evaluate(user_query, rubric, iterations=2)\n",
    "table = eval.score_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd5bc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- USER QUERY ANSWERS --------------------------------\n",
      "\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"model\": \"TNG: DeepSeek R1T2 Chimera (free)\",\n",
      "        \"response\": \"\\n\\nFor **1080p gaming**, the ideal amount of VRAM depends on your gaming preferences, settings, and future-proofing goals. Here's a concise breakdown:\\n\\n### **Minimum Recommendation**\\n- **6GB VRAM**  \\n  - *Suitable for*: Older or less demanding games, esports titles (e.g., CS:GO, Valorant, Fortnite), or medium settings in modern AAA games.  \\n  - *Risk*: May struggle with newer AAA titles at **Ultra settings** or with mods/texture packs.\\n\\n---\\n\\n### **Sweet Spot for Most Gamers**\\n- **8GB VRAM**  \\n  - *Why*: Modern AAA games (e.g., *Cyberpunk 2077*, *Call of Duty*, *Elden Ring*) often use **6\\u20138GB+** at Ultra settings.  \\n  - *Benefits*: Future-proofing for upcoming games, support for high-res textures, and headroom for multitasking (e.g., streaming).  \\n  - *GPU Examples*: RTX 3050/3060, RX 6600/7600, Arc A580.\\n\\n---\\n\\n### **Future-Proof or High-Demand Use Cases**\\n- **12GB VRAM**  \\n  - *Consider if*: You play **heavily modded games** (e.g., *Skyrim*, *Minecraft*), use ray tracing, or plan to keep your GPU for **4+ years**.  \\n  - *GPU Examples*: RTX 4070, RX 7700 XT, or higher.\\n\\n---\\n\\n### **Key Factors to Consider**\\n1. **Game Settings**: Ultra textures, ray tracing, or anti-aliasing (e.g., 8K textures in *Horizon Zero Dawn*) can push VRAM usage **beyond 8GB** even at 1080p.\\n2. **Ray Tracing**: Enabling RT often adds **1\\u20132GB+** of VRAM overhead.\\n3. **Future Games**: Titles like *GTA 6* or *Unreal Engine 5 games* will likely demand more VRAM.\\n4. **VRAM vs. GPU Power**: A weak GPU with 12GB VRAM won\\u2019t outperform a strong 8GB GPU (e.g., RTX 3060 12GB vs. RTX 4060 8GB). Balance specs!\\n\\n---\\n\\n### **TL;DR**\\n- **6GB**: Budget option (may require lowering settings).  \\n- **8GB**: Ideal for most 1080p gamers today.  \\n- **12GB**: Overkill for 1080p now, but safer long-term.  \\n\\nAlways pair VRAM with a **strong enough GPU** (e.g., RTX 3060/4060 or RX 6600 XT/7600). Check benchmarks for specific games! \\ud83c\\udfae\"\n",
      "    },\n",
      "    {\n",
      "        \"model\": \"StepFun: Step 3.5 Flash (free)\",\n",
      "        \"response\": \"Excellent question! For 1080p (1920 x 1080) gaming in **2024 and beyond**, the VRAM recommendation depends heavily on your goals: **minimum playable, optimal settings, or future-proofing.**\\n\\nHere\\u2019s a detailed breakdown:\\n\\n### The General Guideline (The \\\"Sweet Spot\\\")\\nFor a great experience at High to Ultra settings in modern AAA titles, including those with detailed textures and ray tracing, you should target:\\n\\n**8GB of VRAM** is the current **recommended minimum/sweet spot** for 1080p gaming.\\n\\n*   **Why?** Modern game engines (Unreal Engine 5, Frostbite) and art styles use extremely high-resolution texture packs. A game like *Cyberpunk 2077* or *Alan Wake 2* can easily use 6-8GB of VRAM at 1080p with Ultra textures.\\n*   **GPU Examples:** NVIDIA RTX 4060 (8GB), AMD Radeon RX 7600 (8GB), Intel Arc A770 (16GB, but 8GB variant exists).\\n\\n---\\n\\n### Detailed Breakdown by Scenario\\n\\n| Your Goal / Use Case | Recommended VRAM | Reasoning & Notes |\\n| :--- | :--- | :--- |\\n| **Competitive Esports** (CS2, Valorant, Fortnite, Rocket League) | **4GB - 6GB** | These games areoptimized for high frame rates, not max textures. 6GB is very comfortable. 4GB (e.g., GTX 1650 Super) can struggle with texture-heavy modern games. |\\n| **Mainstream 1080p Gaming** (High/Ultra settings in AAA titles) | **8GB (Strongly Recommended)** | The safe, modern standard. Allows you to use High textures without stuttering in most 2022-2024 games. Essential for good performance with **FSR/DLSS Quality mode**. |\\n| **Future-Proofing for 1080p** (5+ years) | **12GB+** | Games will only need more VRAM. Cards like the **RTX 4060 Ti 16GB** or **RX 7700 XT 12GB** give you headroom for future texture packs and mods, avoiding VRAM limits as the primary bottleneck. |\\n| **1080p with Ray Tracing** | **10GB+** | Ray tracing adds a significant VRAM overhead. To run RT effects smoothly at 1080p, you'll want a card with more VRAM (like an RTX 4060 Ti 16GB or RX 7700 XT) to compensate for the performance cost. |\\n| **Playing at 1440p (2K) on a 1080p monitor?** (Upscaling from higher res) | **8GB is okay, 12GB+ better** | If you use DLSS/FSR to render internally at 1440p and downscale to 1080p for better image quality, you're using the VRAM of a 1440p game. This pushes VRAM requirements closer to 1440p levels. |\\n\\n---\\n\\n### Critical Factors That Affect VRAM Usage\\n\\n1.  **Texture Quality/Filtering:** This is the **#1 consumer of VRAM**. \\\"Ultra\\\" texture settings can double the VRAM usage vs. \\\"High.\\\"\\n2.  **Resolution Scaling / Upscaling (DLSS, FSR, XeSS):** Using **Performance** mode reduces VRAM use. Using **Quality** or **Native** resolution increases it.\\n3.  **Ray Tracing & Path Tracing:** These features dramatically increase the memory footprint.\\n4.  **Mods:** Texture replacement mods for games like *Skyrim*, *Fallout*, or *Minecraft* (with RTX) can consume **10-15GB+** of VRAM on their own.\\n5.  **Game-Specific Optimization:** Some games (*Doom Eternal*, *Forza Horizon 5*) are incredibly efficient. Others (*Starfield*, *Resident Evil 4 Remake*) can be VRAM-hungry even at 1080p.\\n\\n---\\n\\n### **Practical Buying Advice for 1080p in 2024**\\n\\n*   **Budget Builder / Entry-Level:** **6GB is the absolute minimum** you should consider today (e.g., used RTX 2060, RX 6600). It will handle esports and older/well-optimized AAA titles at 1080p.\\n*   **The Smart, Balanced Choice:** **Buy an 8GB card.** This is the best value for a hassle-free experience with new releases for the next 2-3 years. (RTX 4060, RX 7600, RX 6700 XT).\\n*   **The Future-Proof / Enthusiast Choice:** **Aim for 12GB or 16GB.** This is overkill for pure 1080p *today*, but it guarantees you won't hit a VRAM ceiling for a very long time, especially if you dabble in mods or enable all the visual bells and whistles. The VRAM headroom becomes your new \\\"performance buffer.\\\" (RX 7700 XT 12GB, RTX 4060 Ti 16GB).\\n\\n### **Red Flags to Avoid**\\n*   GPUs with **4GB of VRAM** (except for very specific, cheap esports builds). They will **struggle** with modern AAA titles at 1080p, forcing you to lower textures and causing potential stutters as texture data swaps in and out of slower RAM.\\n*   **Do not buy a GPU based solely on raw TFLOPS or clock speed.** A card with 25% more shader power but 4GB VRAM will often perform worse in a modern game than a card with less power but 8GB VRAM, because it will be **starved for texture data**.\\n\\n**Final Verdict:** For most gamers building or buying a new system today, **target 8GB of VRAM as your standard for 1080p.** If your budget allows and you want to be absolutely safe for the next console generation's lifecycle, **spending more for 12GB/16GB is a wise long-term investment.**\"\n",
      "    },\n",
      "    {\n",
      "        \"model\": \"AllenAI: Molmo2 8B (free)\",\n",
      "        \"response\": \"For 1080p gaming in VR, it's generally recommended to have at least 6GB of VRAM for high-quality experiences, especially with demanding titles. However, the actual amount of VRAM needed can depend on a number of factors, including the specific VR headset you're using and the games you intend to play. It's always a good idea to check the requirements of the VR games you're interested in playing. Please remember to use this information responsibly and within ethical guidelines.\"\n",
      "    }\n",
      "]\n",
      "\n",
      "\n",
      "-------------------------------- SCORING RESULTS --------------------------------\n",
      "\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"evaluated_model\": \"StepFun: Step 3.5 Flash (free)\",\n",
      "        \"evaluating_model\": \"TNG: DeepSeek R1T2 Chimera (free)\",\n",
      "        \"score\": 99,\n",
      "        \"reasoning\": \"The response achieves full Correctness & Accuracy (25/25) with no factual errors, ensuring foundationally reliable information. Completeness (20/20) is met via comprehensive scenario breakdowns and technical factors. Clarity & Coherence (18/18) is demonstrated through structured tables, bullet points, and logical flow. Relevance (18/18) is strict with zero tangentials. Conciseness earns 9/10 for minor repetition in '8GB is sweet spot' justification. Appropriateness (9/9) fits both technical depth and accessible explanations.\"\n",
      "    },\n",
      "    {\n",
      "        \"evaluated_model\": \"AllenAI: Molmo2 8B (free)\",\n",
      "        \"evaluating_model\": \"TNG: DeepSeek R1T2 Chimera (free)\",\n",
      "        \"score\": 34,\n",
      "        \"reasoning\": \"Response scored 0/25 in Correctness for fundamental factual error (confusing 1080p gaming with VR gaming), 8/20 in Completeness for partial coverage of variables, 15/18 in Clarity for logical structure despite incorrect premise, 2/18 in Relevance for off-topic VR focus, 6/10 in Conciseness, and 3/9 in Appropriateness. Highest penalty from correctness (25% weight) due to hallucinated VR context.\"\n",
      "    },\n",
      "    {\n",
      "        \"evaluated_model\": \"TNG: DeepSeek R1T2 Chimera (free)\",\n",
      "        \"evaluating_model\": \"StepFun: Step 3.5 Flash (free)\",\n",
      "        \"score\": 94,\n",
      "        \"reasoning\": \"The response provides accurate general VRAM guidelines for 10800p gaming (6GB minimum, 8GB sweet spot, 12GB future-proof), but contains factual inaccuracies in GPU examples (RTX 3060 typically has 12GB, not 8GB; RTX 3050 variants vary, with 4GB common), which undermines correctness\\u2014the highest-weighted criterion. All other dimensions (completeness, clarity, relevance, conciseness, appropriateness) are strong and meet rubric standards.\"\n",
      "    },\n",
      "    {\n",
      "        \"evaluated_model\": \"AllenAI: Molmo2 8B (free)\",\n",
      "        \"evaluating_model\": \"StepFun: Step 3.5 Flash (free)\",\n",
      "        \"score\": 28,\n",
      "        \"reasoning\": \"The candidate response fundamentally misinterprets the query as concerning VR gaming rather than standard 1080p gaming, resulting in a critical factual inaccuracy that severely impacts the highest-weighted correctness score, and leading to low completeness, relevance, and appropriateness due to context mismatch and an irrelevant ethical note.\"\n",
      "    },\n",
      "    {\n",
      "        \"evaluated_model\": \"TNG: DeepSeek R1T2 Chimera (free)\",\n",
      "        \"evaluating_model\": \"AllenAI: Molmo2 8B (free)\",\n",
      "        \"score\": 91,\n",
      "        \"reasoning\": \"The response is accurate and well-researched, covering minimum, optimal, and future-proof recommendations for 1080p VRAM; presents logical breakdowns and practical guidance with clear examples, is well-structured and complete, stays fully relevant, avoids unnecessary repetition, and is written with the depth expected for a technical advice query. Correctness & Accuracy is weighted highest and correct here.\"\n",
      "    },\n",
      "    {\n",
      "        \"evaluated_model\": \"StepFun: Step 3.5 Flash (free)\",\n",
      "        \"evaluating_model\": \"AllenAI: Molmo2 8B (free)\",\n",
      "        \"score\": 95,\n",
      "        \"reasoning\": \"The Candidate Response is extremely detailed, appropriate for the technical nature of the question, covers large aspect of VRAM usage for 1080p, provides scenario-based guidance and practical advice, cites model examples, and flags common mistakes. The darkest spot is a minor, non-fatal error in the line 'The Smart, Balanced Choice: Buy an 8GB card', where it mentions '2-3 years' of future-proofing while general industry outlook sometimes prescribes a longer horizon, but the bulk is correct and practical information that matches the question's requirements.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------- USER QUERY ANSWERS --------------------------------\\n\\n\")\n",
    "print(str(json.dumps(eval.user_query_answers, indent=4)))\n",
    "print(\"\\n\\n-------------------------------- SCORING RESULTS --------------------------------\\n\\n\")\n",
    "print(str(json.dumps(eval.evaluation_query_answers, indent=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb4702d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Evaluated Model (Column)</th>\n",
       "      <th>TNG: DeepSeek R1T2 Chimera (free)</th>\n",
       "      <th>StepFun: Step 3.5 Flash (free)</th>\n",
       "      <th>AllenAI: Molmo2 8B (free)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge Model (Row)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TNG: DeepSeek R1T2 Chimera (free)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StepFun: Step 3.5 Flash (free)</th>\n",
       "      <td>94.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AllenAI: Molmo2 8B (free)</th>\n",
       "      <td>91.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Evaluated Model (Column)           TNG: DeepSeek R1T2 Chimera (free)  \\\n",
       "Judge Model (Row)                                                      \n",
       "TNG: DeepSeek R1T2 Chimera (free)                                NaN   \n",
       "StepFun: Step 3.5 Flash (free)                                  94.0   \n",
       "AllenAI: Molmo2 8B (free)                                       91.0   \n",
       "\n",
       "Evaluated Model (Column)           StepFun: Step 3.5 Flash (free)  \\\n",
       "Judge Model (Row)                                                   \n",
       "TNG: DeepSeek R1T2 Chimera (free)                            99.0   \n",
       "StepFun: Step 3.5 Flash (free)                                NaN   \n",
       "AllenAI: Molmo2 8B (free)                                    95.0   \n",
       "\n",
       "Evaluated Model (Column)           AllenAI: Molmo2 8B (free)  \n",
       "Judge Model (Row)                                             \n",
       "TNG: DeepSeek R1T2 Chimera (free)                       34.0  \n",
       "StepFun: Step 3.5 Flash (free)                          28.0  \n",
       "AllenAI: Molmo2 8B (free)                                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab21e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
